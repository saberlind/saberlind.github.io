<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Kafka | Awaken&#39;s blogs</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/wtw.jpg">
    <meta name="description" content="Awaken's blogs">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    
    <link rel="preload" href="/assets/css/0.styles.dfeb017d.css" as="style"><link rel="preload" href="/assets/js/app.03646987.js" as="script"><link rel="preload" href="/assets/js/3.a8134f0a.js" as="script"><link rel="preload" href="/assets/js/1.a394b090.js" as="script"><link rel="preload" href="/assets/js/28.682ad17a.js" as="script"><link rel="prefetch" href="/assets/js/10.0aed174e.js"><link rel="prefetch" href="/assets/js/100.02914097.js"><link rel="prefetch" href="/assets/js/101.95f6c81a.js"><link rel="prefetch" href="/assets/js/102.48df0133.js"><link rel="prefetch" href="/assets/js/103.200f5642.js"><link rel="prefetch" href="/assets/js/104.080c910a.js"><link rel="prefetch" href="/assets/js/105.943605b5.js"><link rel="prefetch" href="/assets/js/106.55d2b7c0.js"><link rel="prefetch" href="/assets/js/107.dbf3105f.js"><link rel="prefetch" href="/assets/js/108.3a747b28.js"><link rel="prefetch" href="/assets/js/109.e9453b81.js"><link rel="prefetch" href="/assets/js/11.38bfbf06.js"><link rel="prefetch" href="/assets/js/110.15ce5269.js"><link rel="prefetch" href="/assets/js/111.e2a2b309.js"><link rel="prefetch" href="/assets/js/112.83e22ec5.js"><link rel="prefetch" href="/assets/js/113.4c53e21e.js"><link rel="prefetch" href="/assets/js/114.3fdb14e5.js"><link rel="prefetch" href="/assets/js/115.24fd8b47.js"><link rel="prefetch" href="/assets/js/116.fa4af7d3.js"><link rel="prefetch" href="/assets/js/117.c5bc84c0.js"><link rel="prefetch" href="/assets/js/118.6ad3d598.js"><link rel="prefetch" href="/assets/js/119.fd9eb4ba.js"><link rel="prefetch" href="/assets/js/12.088628ed.js"><link rel="prefetch" href="/assets/js/120.36824fa8.js"><link rel="prefetch" href="/assets/js/121.6edd4478.js"><link rel="prefetch" href="/assets/js/122.228b556b.js"><link rel="prefetch" href="/assets/js/123.a51f4b7b.js"><link rel="prefetch" href="/assets/js/124.32fa9997.js"><link rel="prefetch" href="/assets/js/125.29138718.js"><link rel="prefetch" href="/assets/js/126.bfeae404.js"><link rel="prefetch" href="/assets/js/127.bbe78f94.js"><link rel="prefetch" href="/assets/js/128.6a9fd1ab.js"><link rel="prefetch" href="/assets/js/129.a26030ce.js"><link rel="prefetch" href="/assets/js/13.50987128.js"><link rel="prefetch" href="/assets/js/130.5a29c5e3.js"><link rel="prefetch" href="/assets/js/131.83dd6a40.js"><link rel="prefetch" href="/assets/js/132.31961e3b.js"><link rel="prefetch" href="/assets/js/133.12379872.js"><link rel="prefetch" href="/assets/js/134.2f630670.js"><link rel="prefetch" href="/assets/js/135.516248e9.js"><link rel="prefetch" href="/assets/js/136.c9fb3676.js"><link rel="prefetch" href="/assets/js/137.0d58b935.js"><link rel="prefetch" href="/assets/js/138.106b8051.js"><link rel="prefetch" href="/assets/js/139.3a4177bb.js"><link rel="prefetch" href="/assets/js/14.d98ee26c.js"><link rel="prefetch" href="/assets/js/140.cf104912.js"><link rel="prefetch" href="/assets/js/141.917d5919.js"><link rel="prefetch" href="/assets/js/142.d0c49c8f.js"><link rel="prefetch" href="/assets/js/143.e5647fcb.js"><link rel="prefetch" href="/assets/js/144.42362cf2.js"><link rel="prefetch" href="/assets/js/145.9f603000.js"><link rel="prefetch" href="/assets/js/146.9b30f1cd.js"><link rel="prefetch" href="/assets/js/147.00acaf92.js"><link rel="prefetch" href="/assets/js/148.0c69b088.js"><link rel="prefetch" href="/assets/js/149.2f219e93.js"><link rel="prefetch" href="/assets/js/15.d642d730.js"><link rel="prefetch" href="/assets/js/150.375474c6.js"><link rel="prefetch" href="/assets/js/151.66a582d5.js"><link rel="prefetch" href="/assets/js/152.1665682e.js"><link rel="prefetch" href="/assets/js/153.5a97da8e.js"><link rel="prefetch" href="/assets/js/154.3b76b78d.js"><link rel="prefetch" href="/assets/js/155.c92a6111.js"><link rel="prefetch" href="/assets/js/156.a247cd62.js"><link rel="prefetch" href="/assets/js/16.172153f0.js"><link rel="prefetch" href="/assets/js/17.ca69a778.js"><link rel="prefetch" href="/assets/js/18.3be7ba8f.js"><link rel="prefetch" href="/assets/js/19.4f66b5d8.js"><link rel="prefetch" href="/assets/js/20.8f0cd2cb.js"><link rel="prefetch" href="/assets/js/21.20681ba8.js"><link rel="prefetch" href="/assets/js/22.53b5baea.js"><link rel="prefetch" href="/assets/js/23.8b55cb0e.js"><link rel="prefetch" href="/assets/js/24.3328372c.js"><link rel="prefetch" href="/assets/js/25.ec23edd8.js"><link rel="prefetch" href="/assets/js/26.76e210a5.js"><link rel="prefetch" href="/assets/js/27.29b300ec.js"><link rel="prefetch" href="/assets/js/29.e1db4b89.js"><link rel="prefetch" href="/assets/js/30.796c2e36.js"><link rel="prefetch" href="/assets/js/31.912d2fae.js"><link rel="prefetch" href="/assets/js/32.60a749df.js"><link rel="prefetch" href="/assets/js/33.d87002b1.js"><link rel="prefetch" href="/assets/js/34.40a8b3f6.js"><link rel="prefetch" href="/assets/js/35.670ac802.js"><link rel="prefetch" href="/assets/js/36.e9f74d88.js"><link rel="prefetch" href="/assets/js/37.6d6f43d0.js"><link rel="prefetch" href="/assets/js/38.ef496ae1.js"><link rel="prefetch" href="/assets/js/39.fc216f43.js"><link rel="prefetch" href="/assets/js/4.01043612.js"><link rel="prefetch" href="/assets/js/40.3bd13625.js"><link rel="prefetch" href="/assets/js/41.3f331b95.js"><link rel="prefetch" href="/assets/js/42.3b26f469.js"><link rel="prefetch" href="/assets/js/43.744d42da.js"><link rel="prefetch" href="/assets/js/44.8b9a8908.js"><link rel="prefetch" href="/assets/js/45.19c8ed8e.js"><link rel="prefetch" href="/assets/js/46.29927647.js"><link rel="prefetch" href="/assets/js/47.e4f53070.js"><link rel="prefetch" href="/assets/js/48.108272d7.js"><link rel="prefetch" href="/assets/js/49.c1e35862.js"><link rel="prefetch" href="/assets/js/5.80b3dd9d.js"><link rel="prefetch" href="/assets/js/50.8af695e4.js"><link rel="prefetch" href="/assets/js/51.33ef3df2.js"><link rel="prefetch" href="/assets/js/52.58bb8f89.js"><link rel="prefetch" href="/assets/js/53.4c6b5e8a.js"><link rel="prefetch" href="/assets/js/54.5763a55d.js"><link rel="prefetch" href="/assets/js/55.15295e7d.js"><link rel="prefetch" href="/assets/js/56.61c94966.js"><link rel="prefetch" href="/assets/js/57.46def60f.js"><link rel="prefetch" href="/assets/js/58.119434b5.js"><link rel="prefetch" href="/assets/js/59.f712c9c5.js"><link rel="prefetch" href="/assets/js/6.a702a72d.js"><link rel="prefetch" href="/assets/js/60.2072c323.js"><link rel="prefetch" href="/assets/js/61.09518814.js"><link rel="prefetch" href="/assets/js/62.35a494eb.js"><link rel="prefetch" href="/assets/js/63.d31876c6.js"><link rel="prefetch" href="/assets/js/64.8f68a737.js"><link rel="prefetch" href="/assets/js/65.6b094c4b.js"><link rel="prefetch" href="/assets/js/66.7c5599cd.js"><link rel="prefetch" href="/assets/js/67.625cd24c.js"><link rel="prefetch" href="/assets/js/68.371f4f22.js"><link rel="prefetch" href="/assets/js/69.c45293ee.js"><link rel="prefetch" href="/assets/js/7.076cee01.js"><link rel="prefetch" href="/assets/js/70.7a22bd02.js"><link rel="prefetch" href="/assets/js/71.729213ff.js"><link rel="prefetch" href="/assets/js/72.0f07bfbc.js"><link rel="prefetch" href="/assets/js/73.4d0355d4.js"><link rel="prefetch" href="/assets/js/74.0f47f56e.js"><link rel="prefetch" href="/assets/js/75.d11ecf15.js"><link rel="prefetch" href="/assets/js/76.727f8a27.js"><link rel="prefetch" href="/assets/js/77.f71a8653.js"><link rel="prefetch" href="/assets/js/78.cb1a43c1.js"><link rel="prefetch" href="/assets/js/79.9e83573b.js"><link rel="prefetch" href="/assets/js/8.d5f02645.js"><link rel="prefetch" href="/assets/js/80.ffafa3ff.js"><link rel="prefetch" href="/assets/js/81.30de8c9b.js"><link rel="prefetch" href="/assets/js/82.4cb0a9a6.js"><link rel="prefetch" href="/assets/js/83.6bf25e9a.js"><link rel="prefetch" href="/assets/js/84.984a50d8.js"><link rel="prefetch" href="/assets/js/85.81009c16.js"><link rel="prefetch" href="/assets/js/86.3325dd7c.js"><link rel="prefetch" href="/assets/js/87.d9dea564.js"><link rel="prefetch" href="/assets/js/88.620d3c46.js"><link rel="prefetch" href="/assets/js/89.414375c4.js"><link rel="prefetch" href="/assets/js/9.f9048235.js"><link rel="prefetch" href="/assets/js/90.07be05fd.js"><link rel="prefetch" href="/assets/js/91.5b6132ee.js"><link rel="prefetch" href="/assets/js/92.efd68e29.js"><link rel="prefetch" href="/assets/js/93.4e37aebc.js"><link rel="prefetch" href="/assets/js/94.a306e6d9.js"><link rel="prefetch" href="/assets/js/95.ddbaa2e0.js"><link rel="prefetch" href="/assets/js/96.b4c98c12.js"><link rel="prefetch" href="/assets/js/97.3bca48ba.js"><link rel="prefetch" href="/assets/js/98.f52f497a.js"><link rel="prefetch" href="/assets/js/99.588f017d.js">
    <link rel="stylesheet" href="/assets/css/0.styles.dfeb017d.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-1156296a><div data-v-1156296a><div id="loader-wrapper" class="loading-wrapper" data-v-d48f4d20 data-v-1156296a data-v-1156296a><div class="loader-main" data-v-d48f4d20><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div></div> <!----> <!----></div> <div class="password-shadow password-wrapper-out" style="display:none;" data-v-4e82dffc data-v-1156296a data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>Awaken's blogs</h3> <p class="description" data-v-4e82dffc data-v-4e82dffc>Awaken's blogs</p> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>Awaken</span>
            
          <!---->
          2024
        </a></span></div></div> <div class="hide" data-v-1156296a><header class="navbar" data-v-1156296a><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/wtw.jpg" alt="Awaken's blogs" class="logo"> <span class="site-name">Awaken's blogs</span></a> <div class="links"><div class="color-picker"><a class="color-button"><i class="iconfont reco-color"></i></a> <div class="color-picker-menu" style="display:none;"><div class="mode-options"><h4 class="title">Choose mode</h4> <ul class="color-mode-options"><li class="dark">dark</li><li class="auto active">auto</li><li class="light">light</li></ul></div></div></div> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/BigData/" class="nav-link"><i class="undefined"></i>
  BigData
</a></li><li class="dropdown-item"><!----> <a href="/categories/DAG/" class="nav-link"><i class="undefined"></i>
  DAG
</a></li><li class="dropdown-item"><!----> <a href="/categories/ES/" class="nav-link"><i class="undefined"></i>
  ES
</a></li><li class="dropdown-item"><!----> <a href="/categories/Flink/" class="nav-link"><i class="undefined"></i>
  Flink
</a></li><li class="dropdown-item"><!----> <a href="/categories/JUC/" class="nav-link"><i class="undefined"></i>
  JUC
</a></li><li class="dropdown-item"><!----> <a href="/categories/JVM/" class="nav-link"><i class="undefined"></i>
  JVM
</a></li><li class="dropdown-item"><!----> <a href="/categories/JavaSE/" class="nav-link"><i class="undefined"></i>
  JavaSE
</a></li><li class="dropdown-item"><!----> <a href="/categories/Java/" class="nav-link"><i class="undefined"></i>
  Java
</a></li><li class="dropdown-item"><!----> <a href="/categories/LeetCode/" class="nav-link"><i class="undefined"></i>
  LeetCode
</a></li><li class="dropdown-item"><!----> <a href="/categories/LINUX/" class="nav-link"><i class="undefined"></i>
  LINUX
</a></li><li class="dropdown-item"><!----> <a href="/categories/Linux/" class="nav-link"><i class="undefined"></i>
  Linux
</a></li><li class="dropdown-item"><!----> <a href="/categories/DB/" class="nav-link"><i class="undefined"></i>
  DB
</a></li><li class="dropdown-item"><!----> <a href="/categories/SSM/" class="nav-link"><i class="undefined"></i>
  SSM
</a></li><li class="dropdown-item"><!----> <a href="/categories/RK/" class="nav-link"><i class="undefined"></i>
  RK
</a></li><li class="dropdown-item"><!----> <a href="/categories/MQ/" class="nav-link"><i class="undefined"></i>
  MQ
</a></li><li class="dropdown-item"><!----> <a href="/categories/redis/" class="nav-link"><i class="undefined"></i>
  redis
</a></li><li class="dropdown-item"><!----> <a href="/categories/Spark/" class="nav-link"><i class="undefined"></i>
  Spark
</a></li><li class="dropdown-item"><!----> <a href="/categories/SpringBoot/" class="nav-link"><i class="undefined"></i>
  SpringBoot
</a></li><li class="dropdown-item"><!----> <a href="/categories/projects/" class="nav-link"><i class="undefined"></i>
  projects
</a></li><li class="dropdown-item"><!----> <a href="/categories/SpringCloud/" class="nav-link"><i class="undefined"></i>
  SpringCloud
</a></li><li class="dropdown-item"><!----> <a href="/categories/project1/" class="nav-link"><i class="undefined"></i>
  project1
</a></li><li class="dropdown-item"><!----> <a href="/categories/Tools/" class="nav-link"><i class="undefined"></i>
  Tools
</a></li><li class="dropdown-item"><!----> <a href="/categories/arthas/" class="nav-link"><i class="undefined"></i>
  arthas
</a></li><li class="dropdown-item"><!----> <a href="/categories/deploy/" class="nav-link"><i class="undefined"></i>
  deploy
</a></li><li class="dropdown-item"><!----> <a href="/categories/k8s/" class="nav-link"><i class="undefined"></i>
  k8s
</a></li><li class="dropdown-item"><!----> <a href="/categories/mall/" class="nav-link"><i class="undefined"></i>
  mall
</a></li><li class="dropdown-item"><!----> <a href="/categories/Python/" class="nav-link"><i class="undefined"></i>
  Python
</a></li><li class="dropdown-item"><!----> <a href="/categories/others/" class="nav-link"><i class="undefined"></i>
  others
</a></li><li class="dropdown-item"><!----> <a href="/categories/Spring/" class="nav-link"><i class="undefined"></i>
  Spring
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Docs
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/docs/documents/" class="nav-link"><i class="undefined"></i>
  documents
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/recoluan" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-1156296a></div> <aside class="sidebar" data-v-1156296a><div class="personal-info-wrapper" data-v-828910c6 data-v-1156296a><img src="/wtw.jpg" alt="author-avatar" class="personal-img" data-v-828910c6> <h3 class="name" data-v-828910c6>
    Awaken
  </h3> <div class="num" data-v-828910c6><div data-v-828910c6><h3 data-v-828910c6>144</h3> <h6 data-v-828910c6>Articles</h6></div> <div data-v-828910c6><h3 data-v-828910c6>78</h3> <h6 data-v-828910c6>Tags</h6></div></div> <ul class="social-links" data-v-828910c6></ul> <hr data-v-828910c6></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/BigData/" class="nav-link"><i class="undefined"></i>
  BigData
</a></li><li class="dropdown-item"><!----> <a href="/categories/DAG/" class="nav-link"><i class="undefined"></i>
  DAG
</a></li><li class="dropdown-item"><!----> <a href="/categories/ES/" class="nav-link"><i class="undefined"></i>
  ES
</a></li><li class="dropdown-item"><!----> <a href="/categories/Flink/" class="nav-link"><i class="undefined"></i>
  Flink
</a></li><li class="dropdown-item"><!----> <a href="/categories/JUC/" class="nav-link"><i class="undefined"></i>
  JUC
</a></li><li class="dropdown-item"><!----> <a href="/categories/JVM/" class="nav-link"><i class="undefined"></i>
  JVM
</a></li><li class="dropdown-item"><!----> <a href="/categories/JavaSE/" class="nav-link"><i class="undefined"></i>
  JavaSE
</a></li><li class="dropdown-item"><!----> <a href="/categories/Java/" class="nav-link"><i class="undefined"></i>
  Java
</a></li><li class="dropdown-item"><!----> <a href="/categories/LeetCode/" class="nav-link"><i class="undefined"></i>
  LeetCode
</a></li><li class="dropdown-item"><!----> <a href="/categories/LINUX/" class="nav-link"><i class="undefined"></i>
  LINUX
</a></li><li class="dropdown-item"><!----> <a href="/categories/Linux/" class="nav-link"><i class="undefined"></i>
  Linux
</a></li><li class="dropdown-item"><!----> <a href="/categories/DB/" class="nav-link"><i class="undefined"></i>
  DB
</a></li><li class="dropdown-item"><!----> <a href="/categories/SSM/" class="nav-link"><i class="undefined"></i>
  SSM
</a></li><li class="dropdown-item"><!----> <a href="/categories/RK/" class="nav-link"><i class="undefined"></i>
  RK
</a></li><li class="dropdown-item"><!----> <a href="/categories/MQ/" class="nav-link"><i class="undefined"></i>
  MQ
</a></li><li class="dropdown-item"><!----> <a href="/categories/redis/" class="nav-link"><i class="undefined"></i>
  redis
</a></li><li class="dropdown-item"><!----> <a href="/categories/Spark/" class="nav-link"><i class="undefined"></i>
  Spark
</a></li><li class="dropdown-item"><!----> <a href="/categories/SpringBoot/" class="nav-link"><i class="undefined"></i>
  SpringBoot
</a></li><li class="dropdown-item"><!----> <a href="/categories/projects/" class="nav-link"><i class="undefined"></i>
  projects
</a></li><li class="dropdown-item"><!----> <a href="/categories/SpringCloud/" class="nav-link"><i class="undefined"></i>
  SpringCloud
</a></li><li class="dropdown-item"><!----> <a href="/categories/project1/" class="nav-link"><i class="undefined"></i>
  project1
</a></li><li class="dropdown-item"><!----> <a href="/categories/Tools/" class="nav-link"><i class="undefined"></i>
  Tools
</a></li><li class="dropdown-item"><!----> <a href="/categories/arthas/" class="nav-link"><i class="undefined"></i>
  arthas
</a></li><li class="dropdown-item"><!----> <a href="/categories/deploy/" class="nav-link"><i class="undefined"></i>
  deploy
</a></li><li class="dropdown-item"><!----> <a href="/categories/k8s/" class="nav-link"><i class="undefined"></i>
  k8s
</a></li><li class="dropdown-item"><!----> <a href="/categories/mall/" class="nav-link"><i class="undefined"></i>
  mall
</a></li><li class="dropdown-item"><!----> <a href="/categories/Python/" class="nav-link"><i class="undefined"></i>
  Python
</a></li><li class="dropdown-item"><!----> <a href="/categories/others/" class="nav-link"><i class="undefined"></i>
  others
</a></li><li class="dropdown-item"><!----> <a href="/categories/Spring/" class="nav-link"><i class="undefined"></i>
  Spring
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Docs
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/docs/documents/" class="nav-link"><i class="undefined"></i>
  documents
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/recoluan" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-4e82dffc data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>Kafka</h3> <!----> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>Awaken</span>
            
          <!---->
          2024
        </a></span></div></div> <div data-v-1156296a><main class="page"><section><div class="page-title"><h1 class="title">Kafka</h1> <div data-v-1ff7123e><i class="iconfont reco-account" data-v-1ff7123e><span data-v-1ff7123e>Awaken</span></i> <i class="iconfont reco-date" data-v-1ff7123e><span data-v-1ff7123e>4/27/2022</span></i> <!----> <i class="tags iconfont reco-tag" data-v-1ff7123e><span class="tag-item" data-v-1ff7123e>Kafka</span></i></div></div> <div class="theme-reco-content content__default"><h2 id="_1-常用命令"><a href="#_1-常用命令" class="header-anchor">#</a> 1. 常用命令</h2> <blockquote><p>--topic 定义topic名</p> <p>--replication-factor  定义副本数</p> <p>--partitions  定义分区数</p></blockquote> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token comment"># 查看当前服务器所有topic</span>
kafka-topics.sh --bootstrap-server hadoop102:9092 --list
<span class="token comment"># 创建topic</span>
kafka-topics.sh --bootstrap-server hadoop102:9092 --create --replication-factor <span class="token number">2</span> --partitions <span class="token number">1</span> --topic first
kafka-topics.sh --bootstrap-server hadoop102:9092 --create --topic first --partitions <span class="token number">3</span> --replication-factor <span class="token number">2</span>
<span class="token comment"># 删除topic</span>
kafka-topics.sh --bootstrap-server hadoop102:9092 --delete --topic first
<span class="token comment"># 发送消息</span>
kafka-console-producer.sh --broker-list hadoop102:9092 --topic first
<span class="token comment"># 消费消息</span>
kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first
<span class="token comment"># --from-beginning：会把主题中现有的所有的数据都读取出来。</span>
kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first
kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first  --property print.key<span class="token operator">=</span>true  --offset <span class="token number">9</span>  --partition <span class="token number">0</span>

<span class="token comment"># 集群版本</span>
<span class="token comment"># 查看 topic 列表</span>
kafka-topics.sh --zookeeper hadoop102:2181/kafka --list
<span class="token comment"># 创建 topic</span>
kafka-topics.sh --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka  --create --replication-factor <span class="token number">1</span> --partitions <span class="token number">1</span> --topic topic_log
<span class="token comment"># 删除 topic </span>
kafka-topics.sh --delete --zookeeper hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka --topic topic_log
<span class="token comment"># 查看 topic 详情</span>
kafka-topics.sh --zookeeper hadoop102:2181/kafka --describe --topic topic_log
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><blockquote><p>参数：kafka集群地址、消费者组名称、key序列化、value序列化</p></blockquote> <h2 id="_2-kafka压力测试"><a href="#_2-kafka压力测试" class="header-anchor">#</a> 2. Kafka压力测试</h2> <h2 id="_3-kafka-架构"><a href="#_3-kafka-架构" class="header-anchor">#</a> 3. Kafka 架构</h2> <blockquote><p>生产者、Broker、消费者、Zookeeper。
注意：Zookeeper中保存Broker id和controller等信息，但是没有生产者信息。</p></blockquote> <h3 id="_3-1-工作机制"><a href="#_3-1-工作机制" class="header-anchor">#</a> 3.1 工作机制</h3> <h4 id="生产者发送流程"><a href="#生产者发送流程" class="header-anchor">#</a> 生产者发送流程</h4> <p><img src="https://lskyimage-1306894954.cos.ap-nanjing.myqcloud.com/shuffle%2Fproducer.png" alt=""></p> <h4 id="broker总体工作流程"><a href="#broker总体工作流程" class="header-anchor">#</a> Broker总体工作流程</h4> <p><img src="https://lskyimage-1306894954.cos.ap-nanjing.myqcloud.com/shuffle%2Fbroker.png" alt=""></p> <h4 id="消费者组初始化流程"><a href="#消费者组初始化流程" class="header-anchor">#</a> 消费者组初始化流程</h4> <p><img src="https://lskyimage-1306894954.cos.ap-nanjing.myqcloud.com/shuffle%2Fconsumer_init.png" alt=""></p> <h4 id="消费者组详细消费流程"><a href="#消费者组详细消费流程" class="header-anchor">#</a> 消费者组详细消费流程</h4> <p><img src="https://lskyimage-1306894954.cos.ap-nanjing.myqcloud.com/shuffle%2Fconsumer.png" alt=""></p> <h3 id="_3-2-存储机制"><a href="#_3-2-存储机制" class="header-anchor">#</a> 3.2 存储机制</h3> <blockquote><p>​	Kafka中消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。</p> <p>​	一个topic下的每一个分区都单独维护offset，所以分发到不同分区中的数据是不同的数据。消费者的分区维护是一个消费者组一个主题的一个分区维护一个offset。</p> <p>​	topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。</p></blockquote> <p><img src="https://lskyimage-1306894954.cos.ap-nanjing.myqcloud.com/all%2Fkafka_cons.png" alt=""></p> <blockquote><p>​	由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制，将每个partition分为多个segment。每个segment对应两个文件——“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号。例如，first这个topic有三个分区，则其对应的文件夹为first-0,first-1,first-2。</p></blockquote> <h2 id="_4-生产者"><a href="#_4-生产者" class="header-anchor">#</a> 4. 生产者</h2> <blockquote><p>​	Kafka的Producer发送消息采用的是<strong>异步发送</strong>的方式。在消息发送的过程中，涉及到了两个线程——main线程和Sender线程，以及一个线程共享变量——RecordAccumulator。main线程将消息发送给RecordAccumulator，Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker。</p></blockquote> <p>相关参数：
batch.size：只有数据积累到batch.size之后，sender才会发送数据。
linger.ms：如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。</p> <h3 id="_4-1-分区策略"><a href="#_4-1-分区策略" class="header-anchor">#</a> 4.1 分区策略</h3> <blockquote><p>​	Kafka官方为我们实现了三种Partitioner（分区器），分别是DefaultPartitioner（当未指定分区器时候所使用的默认分区器）、UniformStickyPartitioner、RoundRobinPartitioner。</p></blockquote> <p>分区的原因：</p> <ol><li>方便在集群中扩展，每个Partition 可以通过调整以适应它所在的机器，而一个 topic 又可以有多个 Partition组成，因此整个集群就可以适应任意大小的数据了</li> <li>可以提高并发，因为可以以 Partition 为单位读写了</li></ol> <p>分区的原则：</p> <ol><li><p>指明 partition 的情况下，直接将指明的值直接作为 partiton 值；</p></li> <li><p>没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值；</p></li> <li><p>既没有 partition 值又没有 key 值的情况下， kafka采用Sticky Partition(黏性分区器)，会随机选择一个分区，并尽可能一直使用该分区，待该分区的batch已满或者已完成，kafka再随机一个分区进行使用.(以前是一条条的轮询，现在是一批次的轮询)</p></li></ol> <p>自定义分区，实现org.apache.kafka.clients.producer.Partitioner 接口，重写 partition 方法来达到自定义分区效果。</p> <p>如果想要实现随机分配：</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token class-name">List</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">PartitionInfo</span><span class="token punctuation">&gt;</span></span> partitions <span class="token operator">=</span> cluster<span class="token punctuation">.</span><span class="token function">partitionsForTopic</span><span class="token punctuation">(</span>topic<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">return</span> <span class="token class-name">ThreadLocalRandom</span><span class="token punctuation">.</span><span class="token function">current</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">nextInt</span><span class="token punctuation">(</span>partitions<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>先计算出该主题总的分区数，然后随机地返回一个小于它的正整数。</p> <p>在项目中，如果希望把MySQL中某张表的数据发送到一个分区。可以以表名为key进行发送。</p> <h3 id="_4-2-数据可靠性保证"><a href="#_4-2-数据可靠性保证" class="header-anchor">#</a> 4.2 数据可靠性保证</h3> <h4 id="_4-2-1-生产者发送数据到-topic-partition-的可靠性保证"><a href="#_4-2-1-生产者发送数据到-topic-partition-的可靠性保证" class="header-anchor">#</a> <strong>4.2.1 生产者发送数据到 topic partition 的可靠性保证</strong></h4> <blockquote><p>​	为保证producer发送的数据，能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后，都需要向producer发送ack（acknowledgement确认收到），如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。</p></blockquote> <p><img src="https://lskyimage-1306894954.cos.ap-nanjing.myqcloud.com/all%2Fkafka_pro.png" alt=""></p> <h4 id="_4-2-2-topic-partition-存储数据的可靠性保证"><a href="#_4-2-2-topic-partition-存储数据的可靠性保证" class="header-anchor">#</a> <strong>4.2.2 Topic partition 存储数据的可靠性保证</strong></h4> <ol><li>副本数据同步策略</li></ol> <table><thead><tr><th>方案</th> <th>优点</th> <th>缺点</th></tr></thead> <tbody><tr><td>半数以上完成同步，就发送ack</td> <td>延迟低</td> <td>选举新的leader时，容忍n台节点的故障，需要2n+1个副本</td></tr> <tr><td>全部完成同步，才发送ack</td> <td>选举新的leader时，容忍n台节点的故障，需要n+1个副本</td> <td>延迟高</td></tr></tbody></table> <p>Kafka选择了第二种方案，原因如下：
①同样为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而Kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。
②虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka的影响较小。</p> <ol start="2"><li>ISR</li></ol> <blockquote><p>​采用第二种方案之后，设想以下情景：leader收到数据，所有follower都开始同步数据，但有一个follower，因为某种故障，迟迟不能与leader进行同步，那leader就要一直等下去，直到它完成同步，才能发送ack。这个问题怎么解决呢？</p> <p>​	Leader维护了一个动态的in-sync replica set (ISR)，意为和leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给producer发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由replica.lag.time.max.ms参数设定。Leader发生故障之后，就会从ISR中选举新的leader。</p></blockquote> <ol start="3"><li>ack应答级别</li></ol> <blockquote><p>​对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等ISR中的follower全部接收成功。</p> <p>​	所以Kafka为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。</p></blockquote> <p>acks参数配置：</p> <ul><li>acks=0，生产者发送过来数据就不管了，可靠性差，效率高；</li> <li>acks=1，生产者发送过来数据Leader应答，可靠性中等，效率中等；</li> <li>acks=-1，生产者发送过来数据Leader和ISR队列里面所有Follwer应答，可靠性高，效率低；</li></ul> <blockquote><p>在生产环境中，acks=0很少使用；acks=1，一般用于传输普通日志，允许丢个别数据；acks=-1，一般用于传输和钱相关的数据，对可靠性要求比较高的场景。</p></blockquote> <p>acks：
​	0：这一操作提供了一个最低的延迟，partition的leader接收到消息还没有写入磁盘就已经返回ack，当leader故障时有可能丢失数据；
​	1： partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会丢失数据；</p> <p>​	-1（all）： partition的leader和follower全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成数据重复。</p> <h4 id="_4-2-3-leader和-follower故障处理细节"><a href="#_4-2-3-leader和-follower故障处理细节" class="header-anchor">#</a> 4.2.3 leader和 follower故障处理细节</h4> <p>Log 文件中的 HW 和 LEO</p> <p>LEO（Log End Offset）：每个副本的最后一个 offset</p> <p>HW（High Watermark）：所有副本中最小的 LEO</p> <p><img src="https://lskyimage-1306894954.cos.ap-nanjing.myqcloud.com/all%2Fkafka_leo_hw.png" alt=""></p> <ol><li>follower 故障</li></ol> <blockquote><p>​follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该follower的LEO大于等于该Partition的HW，即follower追上leader之后，就可以重新加入ISR了。</p></blockquote> <ol start="2"><li>leader 故障</li></ol> <blockquote><p>​leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。
<strong>注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</strong></p></blockquote> <h4 id="_4-2-4-broker-角度"><a href="#_4-2-4-broker-角度" class="header-anchor">#</a> 4.2.4 Broker 角度</h4> <p>副本数大于等于2。</p> <p>min.insync.replicas大于等于2。</p> <h3 id="_4-3-exactly-once-语义"><a href="#_4-3-exactly-once-语义" class="header-anchor">#</a> 4.3 Exactly Once 语义</h3> <p>​	将服务器的ACK级别设置为-1，可以保证Producer到Server之间不会丢失数据，即<strong>At Least Once</strong>语义。相对的，将服务器ACK级别设置为0，可以保证生产者每条消息只会被发送一次，即<strong>At Most Once</strong>语义。</p> <p>​	<strong>At Least Once</strong>可以保证数据不丢失，但是不能保证数据不重复；相对的，<strong>At Most Once</strong>可以保证数据不重复，但是不能保证数据不丢失。但是，对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即Exactly Once语义。在0.11版本以前的Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。</p> <p>​	0.11版本的Kafka，引入了一项重大特性：幂等性。所谓的幂等性就是指Producer不论向Server发送多少次重复数据，Server端都只会持久化一条。幂等性结合At Least Once语义，就构成了Kafka的Exactly Once语义。即：<code>At Least Once + 幂等性 = Exactly Once</code></p> <p>​	要启用幂等性，只需要将Producer的参数中enable.idempotence设置为true即可。Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一Partition的消息会附带Sequence Number。而Broker端会对<code>&lt;PID, Partition, SeqNumber&gt;</code>做缓存，当具有相同主键的消息提交时，Broker只会持久化一条。</p> <p>​	但是PID重启就会变化，同时不同的Partition也具有不同主键，所以幂等性无法保证跨分区跨会话的Exactly Once。</p> <h3 id="_4-4-producer-事务"><a href="#_4-4-producer-事务" class="header-anchor">#</a> 4.4 Producer 事务</h3> <p>​	0.11版本的Kafka同时引入了事务的特性，为了实现跨分区跨会话的事务，需要引入一个全局唯一的Transaction ID，并将Producer获得的PID和Transaction ID绑定。这样当Producer重启后就可以通过正在进行的Transaction ID获得原来的PID。
​	为了管理Transaction，Kafka引入了一个新的组件Transaction Coordinator。Producer就是通过和Transaction Coordinator交互获得Transaction ID对应的任务状态。Transaction Coordinator还负责将事务所有写入Kafka的一个内部Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。</p> <h2 id="_5-消费者"><a href="#_5-消费者" class="header-anchor">#</a> 5. 消费者</h2> <h3 id="_5-1-消费方式"><a href="#_5-1-消费方式" class="header-anchor">#</a> 5.1 消费方式</h3> <p>​	consumer采用pull（拉）模式从broker中读取数据。
​	push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。
​	pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout。</p> <h3 id="_5-2-分区分配策略"><a href="#_5-2-分区分配策略" class="header-anchor">#</a> 5.2 分区分配策略</h3> <blockquote><p>​	一个consumer group中有多个consumer，一个 topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费。
​	Kafka有两种分配策略，RoundRobin，Range。</p></blockquote> <p>7个分区：0123456</p> <ol><li>Range</li></ol> <p>consumer1：0,1,2、consumer2：3,4、consumer：5,6</p> <p>​	默认使用Range的分区分配策略，可以通过参数&quot;partition.assignment.strategy&quot;的值进行修改，可以使用多个分区分配策略。
<strong>注意：3个消费者都应该修改分区分配策略，避免出现错误，如果重启失败，则全部停止消费者等一会再启动即可</strong></p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token comment">// 修改分区分配策略</span>
properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>PARTITION_ASSIGNMENT_STRATEGY_CONFIG<span class="token punctuation">,</span> <span class="token string">&quot;org.apache.kafka.clients.consumer.RoundRobinAssignor&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><ol start="2"><li>RoundRobin</li></ol> <p>consumer1：3,6,0、consumer2：2,5、consumer3：4,1</p> <ol start="3"><li>Sticky</li></ol> <blockquote><p>​特殊的分配策略StickyAssignor，Kafka从0.11.x版本开始引入这种分配策略，在出现同一消费者组内消费者出现问题的时候，会进行使用。</p></blockquote> <p>①在上个示例基础上，停止2号消费者，重新发送500条消息，可以看到分区重新进行了划分，此时没有使用粘性分区器。</p> <p>consumer1：4,2,0,6、consumer3：1,3,5</p> <p>②修改分区分配策略</p> <p>注意：3个消费者都应该注释掉，之后重启3个消费者，如果出现报错，全部停止等会再重启，或者修改为全新的消费者组</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token comment">// 修改分区分配策略</span>
<span class="token class-name">ArrayList</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> strings <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">ArrayList</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
strings<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span><span class="token string">&quot;org.apache.kafka.clients.consumer.RoundRobinAssignor&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
strings<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span><span class="token string">&quot;org.apache.kafka.clients.consumer.StickyAssignor&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>PARTITION_ASSIGNMENT_STRATEGY_CONFIG<span class="token punctuation">,</span> strings<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">//粘性分区一般是与主分区策略配合使用的时候需要我们注意 有时候会不起作用。所以尽量避免同时使用</span>
<span class="token comment">//我们呢在使用kafka的过程中 需要我们注意不要频繁的改动的我们的分区策略！</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>③使用同样的生产者发送500条消息</p> <p>按照 RoundRobin 规则划分分区</p> <p>consumer1：0,3,6、consumer2：5,2、consumer3：4,1</p> <p>停止掉一号消费者</p> <p>consumer2：5,2,3、consumer3：1,4,6,0</p> <h3 id="_5-3-offset的维护"><a href="#_5-3-offset的维护" class="header-anchor">#</a> 5.3 offset的维护</h3> <p>​	由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。
​	Kafka 0.9版本之前，consumer默认将offset保存在Zookeeper中，从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为__consumer_offsets。</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token comment">// 不排除内部offset,不然看不到__consumer_offsets</span>
properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>EXCLUDE_INTERNAL_TOPICS_CONFIG<span class="token punctuation">,</span><span class="token string">&quot;false&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>启动消费者消费主题</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>bin/kafka-console-consumer.sh --topic __consumer_offsets --bootstrap-server  hadoop102:9092 --consumer.config config/consumer.properties  --formatter <span class="token string">&quot;kafka.coordinator.group.GroupMetadataManager\<span class="token variable">$OffsetsMessageFormatter</span>&quot;</span> --from-beginning
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>查询消费者组信息</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token comment">## 查询指定消费者组的信息(包含offset)</span>
kafka-consumer-groups.sh  --bootstrap-server hadoop102:9092 --group <span class="token builtin class-name">test</span>  --describe
<span class="token comment">## 查询所有组所有topic的信息</span>
kafka-consumer-groups.sh --all-groups --all-topics --describe --bootstrap-server hadoop102:9092
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><h3 id="_5-4-自动提交offset"><a href="#_5-4-自动提交offset" class="header-anchor">#</a> 5.4 自动提交offset</h3> <p>enable.auto.commit：是否开启自动提交offset功能
auto.commit.interval.ms：自动提交offset的时间间隔</p> <h3 id="_5-5-重置offset"><a href="#_5-5-重置offset" class="header-anchor">#</a> 5.5 重置offset</h3> <p>auto.offset.reset = earliest | latest | none |
当Kafka中没有初始偏移量（消费者组第一次消费）或服务器上不再存在当前偏移量时（例如该数据已被删除），该怎么办：
（1）earliest：自动将偏移量重置为最早的偏移量
（2）latest(默认值)：自动将偏移量重置为最新偏移量
（3）none：如果未找到消费者组的先前偏移量，则向消费者抛出异常</p> <h3 id="_5-6-手动提交offset"><a href="#_5-6-手动提交offset" class="header-anchor">#</a> 5.6 手动提交offset</h3> <p>​	虽然自动提交offset十分简介便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因此Kafka还提供了手动提交offset的API。
​	手动提交offset的方法有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。两者的相同点是，都会将本次poll的一批数据最高的偏移量提交；不同点是，commitSync阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而commitAsync则没有失败重试机制，故有可能提交失败。
1）同步提交offset
由于同步提交offset有失败重试机制，故更加可靠，以下为同步提交offset的示例。</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">CustomConsumerByHand</span> <span class="token punctuation">{</span>

    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token comment">// 1. 创建kafka消费者配置类</span>
        <span class="token class-name">Properties</span> properties <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Properties</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 2. 添加配置参数</span>
        <span class="token comment">// 添加连接</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>BOOTSTRAP_SERVERS_CONFIG<span class="token punctuation">,</span> <span class="token string">&quot;hadoop102:9092&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// 配置序列化 必须</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>KEY_DESERIALIZER_CLASS_CONFIG<span class="token punctuation">,</span> <span class="token string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>VALUE_DESERIALIZER_CLASS_CONFIG<span class="token punctuation">,</span> <span class="token string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 配置消费者组</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>GROUP_ID_CONFIG<span class="token punctuation">,</span> <span class="token string">&quot;test&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 是否自动提交offset</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>ENABLE_AUTO_COMMIT_CONFIG<span class="token punctuation">,</span> <span class="token string">&quot;false&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 提交offset的时间周期</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>AUTO_COMMIT_INTERVAL_MS_CONFIG<span class="token punctuation">,</span> <span class="token string">&quot;1000&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">//3. 创建kafka消费者</span>
        <span class="token class-name">KafkaConsumer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> consumer <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">KafkaConsumer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span>properties<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">//4. 设置消费主题  形参是列表</span>
        consumer<span class="token punctuation">.</span><span class="token function">subscribe</span><span class="token punctuation">(</span><span class="token class-name">Arrays</span><span class="token punctuation">.</span><span class="token function">asList</span><span class="token punctuation">(</span><span class="token string">&quot;first&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">//5. 消费数据</span>
        <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
            <span class="token comment">// 读取消息</span>
            <span class="token class-name">ConsumerRecords</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> consumerRecords <span class="token operator">=</span> consumer<span class="token punctuation">.</span><span class="token function">poll</span><span class="token punctuation">(</span><span class="token class-name">Duration</span><span class="token punctuation">.</span><span class="token function">ofSeconds</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

            <span class="token comment">// 输出消息</span>
            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token class-name">ConsumerRecord</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> consumerRecord <span class="token operator">:</span> consumerRecords<span class="token punctuation">)</span> <span class="token punctuation">{</span>

                <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>consumerRecord<span class="token punctuation">.</span><span class="token function">value</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

            <span class="token punctuation">}</span> 
<span class="token comment">// 同步提交offset</span>
            consumer<span class="token punctuation">.</span><span class="token function">commitSync</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>

    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br></div></div><p>2）异步提交offset
​	虽然同步提交offset更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会受到很大的影响。因此更多的情况下，会选用异步提交offset的方式。
​	以下为异步提交offset的示例：</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">CustomConsumerByHand</span> <span class="token punctuation">{</span>

    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token comment">// 1. 创建kafka消费者配置类</span>
        <span class="token class-name">Properties</span> properties <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Properties</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 2. 添加配置参数</span>
        <span class="token comment">// 添加连接</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>BOOTSTRAP_SERVERS_CONFIG<span class="token punctuation">,</span> <span class="token string">&quot;hadoop102:9092&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// 配置序列化 必须</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>KEY_DESERIALIZER_CLASS_CONFIG<span class="token punctuation">,</span> <span class="token string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>VALUE_DESERIALIZER_CLASS_CONFIG<span class="token punctuation">,</span> <span class="token string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 配置消费者组</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>GROUP_ID_CONFIG<span class="token punctuation">,</span> <span class="token string">&quot;test&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 是否自动提交offset</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>ENABLE_AUTO_COMMIT_CONFIG<span class="token punctuation">,</span> <span class="token string">&quot;false&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 提交offset的时间周期</span>
        properties<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token class-name">ConsumerConfig</span><span class="token punctuation">.</span>AUTO_COMMIT_INTERVAL_MS_CONFIG<span class="token punctuation">,</span> <span class="token string">&quot;1000&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">//3. 创建kafka消费者</span>
        <span class="token class-name">KafkaConsumer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> consumer <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">KafkaConsumer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span>properties<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">//4. 设置消费主题  形参是列表</span>
        consumer<span class="token punctuation">.</span><span class="token function">subscribe</span><span class="token punctuation">(</span><span class="token class-name">Arrays</span><span class="token punctuation">.</span><span class="token function">asList</span><span class="token punctuation">(</span><span class="token string">&quot;first&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">//5. 消费数据</span>
        <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
            <span class="token comment">// 读取消息</span>
            <span class="token class-name">ConsumerRecords</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> consumerRecords <span class="token operator">=</span> consumer<span class="token punctuation">.</span><span class="token function">poll</span><span class="token punctuation">(</span><span class="token class-name">Duration</span><span class="token punctuation">.</span><span class="token function">ofSeconds</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

            <span class="token comment">// 输出消息</span>
            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token class-name">ConsumerRecord</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> consumerRecord <span class="token operator">:</span> consumerRecords<span class="token punctuation">)</span> <span class="token punctuation">{</span>

                <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>consumerRecord<span class="token punctuation">.</span><span class="token function">value</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

            <span class="token punctuation">}</span>
            <span class="token comment">// 同步提交offset</span>
            <span class="token comment">//consumer.commitSync();</span>

            <span class="token comment">// 异步提交offset</span>
            consumer<span class="token punctuation">.</span><span class="token function">commitAsync</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">OffsetCommitCallback</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
                <span class="token comment">/**
                 * 回调函数输出
                 * @param offsets   offset信息
                 * @param exception 异常
                 */</span>
                <span class="token annotation punctuation">@Override</span>
                <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">onComplete</span><span class="token punctuation">(</span><span class="token class-name">Map</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">TopicPartition</span><span class="token punctuation">,</span> <span class="token class-name">OffsetAndMetadata</span><span class="token punctuation">&gt;</span></span> offsets<span class="token punctuation">,</span> <span class="token class-name">Exception</span> exception<span class="token punctuation">)</span> <span class="token punctuation">{</span>
                    <span class="token comment">// 如果出现异常打印</span>
                    <span class="token keyword">if</span> <span class="token punctuation">(</span>exception <span class="token operator">!=</span> <span class="token keyword">null</span> <span class="token punctuation">)</span><span class="token punctuation">{</span>
                        <span class="token class-name">System</span><span class="token punctuation">.</span>err<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">&quot;Commit failed for &quot;</span> <span class="token operator">+</span> offsets<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">else</span><span class="token punctuation">{</span>
                        <span class="token class-name">Set</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">TopicPartition</span><span class="token punctuation">&gt;</span></span> topicPartitions <span class="token operator">=</span> offsets<span class="token punctuation">.</span><span class="token function">keySet</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">//该方法可以得到我们消费的的消息 所处的topic partition 有哪些</span>
                        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token class-name">TopicPartition</span> topicPartition <span class="token operator">:</span> topicPartitions<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token comment">//遍历我们消费的topic 以及parition 元数据</span>
                            <span class="token class-name">OffsetAndMetadata</span> offsetAndMetadata <span class="token operator">=</span> offsets<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>topicPartition<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">//每个topic 的每个parition 的消费到的offset</span>
                            <span class="token keyword">long</span> offset <span class="token operator">=</span> offsetAndMetadata<span class="token punctuation">.</span><span class="token function">offset</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">//获取提交的offset值</span>
                            <span class="token keyword">int</span> partition <span class="token operator">=</span> topicPartition<span class="token punctuation">.</span><span class="token function">partition</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">//获取该parttion的值</span>
                            <span class="token class-name">String</span> topic <span class="token operator">=</span> topicPartition<span class="token punctuation">.</span><span class="token function">topic</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">//获取该toipic的值</span>
                            <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;----提交的offset = %s, 该 partition = %s ,以及topic = %s---------\n&quot;</span><span class="token punctuation">,</span>
                                    offset<span class="token punctuation">,</span>partition<span class="token punctuation">,</span>topic<span class="token punctuation">)</span><span class="token punctuation">;</span>
                        <span class="token punctuation">}</span>


                    <span class="token punctuation">}</span>
                <span class="token punctuation">}</span>
            <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>

    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br><span class="line-number">65</span><br><span class="line-number">66</span><br><span class="line-number">67</span><br><span class="line-number">68</span><br><span class="line-number">69</span><br><span class="line-number">70</span><br></div></div><p>3）数据漏消费和重复消费分析
​	无论是同步提交还是异步提交offset，都有可能会造成数据的漏消费或者重复消费。先提交offset后消费，有可能造成数据的漏消费；而先消费后提交offset，有可能会造成数据的重复消费。</p> <h3 id="_5-7-consumer-事务"><a href="#_5-7-consumer-事务" class="header-anchor">#</a> 5.7 Consumer 事务</h3> <p>​	上述事务机制主要是从Producer方面考虑，对于Consumer而言，事务的保证就会相对较弱，尤其时无法保证Commit的信息被精确消费。这是由于Consumer可以通过offset访问任意信息，而且不同的Segment File生命周期不同，同一事务的消息可能会出现重启后被删除的情况。
​	如果想完成Consumer端的精准一次性消费，那么需要kafka消费端将消费过程和提交offset过程做原子绑定。此时我们需要将kafka的offset保存到支持事务的自定义介质（比如mysql）。</p> <h2 id="_6-kafka-高效读写数据"><a href="#_6-kafka-高效读写数据" class="header-anchor">#</a> 6. Kafka 高效读写数据</h2> <ol><li>分区</li> <li>顺序写磁盘</li></ol> <blockquote><p>​Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到600M/s，而随机写只有100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去 了大量磁头寻址的时间。</p></blockquote> <ol start="3"><li>应用<strong>Pagecache</strong></li></ol> <blockquote><p>Kafka数据持久化是直接持久化到Pagecache中，这样会产生以下几个好处：</p> <ul><li>I/O Scheduler 会将连续的小块写组装成大块的物理写从而提高性能</li> <li>I/O Scheduler 会尝试将一些写操作重新按顺序排好，从而减少磁盘头的移动时间</li> <li>充分利用所有空闲内存（非 JVM 内存）。如果使用应用层 Cache（即 JVM 堆内存），会增加 GC 负担</li> <li>读操作可直接在 Page Cache 内进行。如果消费和生产速度相当，甚至不需要通过物理磁盘（直接通过 Page Cache）交换数据</li> <li>如果进程重启，JVM 内的 Cache 会失效，但 Page Cache 仍然可用，尽管持久化到Pagecache上可能会造成宕机丢失数据的情况，但这可以被Kafka的Replication机制解决。如果为了保证这种情况下数据不丢失而强制将 Page Cache 中的数据 Flush 到磁盘，反而会降低性能。</li></ul></blockquote> <ol start="3"><li>零拷贝技术</li></ol> <blockquote><p>传统拷贝：</p> <p>磁盘----》read buffer-----》application buffer-------》socket buffer---------》网卡-------》发送给消费者</p> <p>零拷贝：</p> <p>磁盘------》readbuff--------》网卡推送</p> <p>​	DMA（Direct Memory Access）技术。DMA，又称之为直接内存访问，是零拷贝技术的基石。DMA 传输将数据从一个地址空间复制到另外一个地址空间。当CPU 初始化这个传输动作，传输动作本身是由 DMA 控制器来实行和完成。因此通过DMA，硬件则可以绕过CPU，自己去直接访问系统主内存。很多硬件都支持DMA，其中就包括网卡、声卡、磁盘驱动控制器等。</p> <p>​	通过零拷贝技术，就不需要把 内核空间页缓存里的数据拷贝到应用层缓存，再从应用层缓存拷贝到 Socket 缓存了，两次拷贝都省略了，所以叫做零拷贝。这个过程大大的提升了数据消费时读取文件数据的性能。Kafka 从磁盘读数据的时候，会先看看内核空间的页缓存中是否有，如果有的话，直接通过网关发送出去。</p></blockquote> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">ZeroCopy</span> <span class="token punctuation">{</span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">IOException</span> <span class="token punctuation">{</span>
        <span class="token class-name">File</span> file <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">File</span><span class="token punctuation">(</span><span class="token string">&quot;xxx.log&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token class-name">RandomAccessFile</span> raf <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">RandomAccessFile</span><span class="token punctuation">(</span>file<span class="token punctuation">,</span> <span class="token string">&quot;rw&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token class-name">FileChannel</span> channel <span class="token operator">=</span> raf<span class="token punctuation">.</span><span class="token function">getChannel</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// Open a socket channel and connects it to a remote address</span>
        <span class="token class-name">SocketChannel</span> socketChannel <span class="token operator">=</span> <span class="token class-name">SocketChannel</span><span class="token punctuation">.</span><span class="token keyword">open</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">InetSocketAddress</span><span class="token punctuation">(</span><span class="token string">&quot;192.168.1.102&quot;</span><span class="token punctuation">,</span> <span class="token number">9091</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// Transfers bytes from this channel's file to the given writable byte channel</span>

        channel<span class="token punctuation">.</span><span class="token function">transferTo</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> channel<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> socketChannel<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><h2 id="_7-总结"><a href="#_7-总结" class="header-anchor">#</a> 7. 总结</h2> <h3 id="_7-1-kafka基础架构"><a href="#_7-1-kafka基础架构" class="header-anchor">#</a> 7.1 Kafka基础架构</h3> <div class="language-txt line-numbers-mode"><pre class="language-txt"><code>producer: 生产者,向topic写消息
topic: 主题,在工作中一般是一个业务一个主题
partition: topic为了实现分布式的存储以及提高生产/消费的吞吐量,将topic划分为多个分区,每个分区保存在不同的节点上
broker: kafka的一个节点
consumer: 消费者,向topic中拉取消息
consumer group: 消费者组,因为一个topic有多个分区,如果只有一个消费者,此时是串行消息,所以为了调高消费的速度,引出了消费者组的概率,此时一个消费者组消费一个topic。
消费者组中有多个消费者,这多个消费者消费topic不同分区的数据。一个分区只能被一个消费者组中的一个消费者所消费

副本: partition因为保存在broker上，所以如果broker宕机,分区数据消失,所以为了保证分区数据的安全性,对每个分区提供了副本机制

leader: 副本中的一个角色,produer写入数据以及消费者消费数据都找分区的leader
follower: 副本中的一个角色,follower同步leader的数据,如果leader宕机,会选举出一个新leader
zookeeper: broker上下线以及leader选举都依赖zookeeper
offset: 数据的偏移量,也相当于是数据唯一标识。消费者消费数据的时候会进行记录,记录下一次应该从哪个offset开始消费
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><h3 id="_7-2-kafka原理"><a href="#_7-2-kafka原理" class="header-anchor">#</a> 7.2 Kafka原理</h3> <h4 id="_1-数据存储机制"><a href="#_1-数据存储机制" class="header-anchor">#</a> 1. 数据存储机制：</h4> <p>Topic: 是逻辑上的概念</p> <ul><li>partition: 物理上真实存在,以目录的形式存在
<ul><li>segment: 相当于是partition的一个分段
<ul><li>log: 数据存储文件</li> <li>index: log文件数据的索引文件</li> <li>timestampindex: log文件数据的时间索引文件</li></ul></li> <li>partition为什么需要切分成多个segment？
<ul><li>partition如果只有一个数据文件和索引文件,那么随着时间的增加,数据文件越来越大,索引文件也越来越大,此时查找数据会越来越慢。所以切分成多个segment能够提高查询效率</li></ul></li> <li>segment文件的命名规则
<ul><li>每个partition第一个segment文件名为00000000000000000000</li> <li>后续第N个segment文件名 = 第 N -1 个segment最后一个offset+1</li> <li>根据segment命名规则,后续查找offset对应的数据的时候,可以根据segment文件名用二分查找法可以很快确定offset数据处于哪个segment</li></ul></li> <li>如果根据offset找到数据?
<ul><li>1、根据segment文件名用二分查找法确定offset处于哪个segment</li> <li>2、再根据segment的index文件确定offset处于log文件哪个区间，其中index 记录的是消息数据的物理地址值 类似于taildir json 数据的</li> <li>3、扫描log文件对应的区间获取数据</li></ul></li></ul></li></ul> <h4 id="_2-生产者"><a href="#_2-生产者" class="header-anchor">#</a> 2. 生产者</h4> <p>1、分区策略[确定数据写到哪个分区]:</p> <ul><li>直接指定分区号: 数据直接发到指定的分区</li> <li>如果没有指定分区号,但是有key: 数据发到 key.hashCode % 分区数 分区</li> <li>没有分区号,也没有指定key:
<ul><li>新版本：
<ul><li>1、第一个批次发送的时候会生成一个随机数, 数据发到 随机数%分区数 分区</li> <li>2、第N个批次发送的时候,会排除掉 N-1 次发送的分区,从剩余的分区中随机选择一个</li></ul></li> <li>旧版本:
<ul><li>1、第一个批次发送的时候会生成一个随机数, 数据发到 随机数%分区数 分区</li> <li>2、第N个批次发送的时候,会将数据发到 (第一个批次生成的随机数+ (N-1))%分区数 分区</li></ul></li></ul></li></ul> <p>2、数据可靠性[生成发送的消息是否真实到达kafka]:</p> <ul><li>通过ack机制[确定消息机制]可以确保数据的可靠性
<ul><li>ack=0:    leader接收到消息之后立即返回确认消息给生产者,此时数据还没有落盘
<ul><li>问题: 如果leader返回了确认消息之后宕机,此时数据因为还没有落盘,所以数据丢失</li></ul></li> <li>ack=1： leader接收到消息并且落盘之后才会返回确认消息给生产者
<ul><li>问题: leader接收到消息并且落盘之后返回确认消息给生产者,返回消息之后宕机了,此时会从follower选举出新leader,新leader中没有该数据,所以也出现了数据丢失</li></ul></li> <li>ack=-1： leader接收到消息并且落盘并且所有的follower全部同步完数据之后才会返回确认消息给生产者
<ul><li>问题: leader接收到消息并且落盘并且所有的follower全部同步完数据,在返回消息之前leader宕机,选举出新leader,因为之前的leader宕机了没有返回确认消息给生产者，所以生产者认为kafka没有接受到消息,此时生产者会重新发送消息给leader,对于新leader来说有两个相同的数据,数据重复</li></ul></li></ul></li> <li>ack=-1的时候要求所有的follower都同步完消息之后才会返回确认消息,所以此时如果某一个follower因为网络故障导致同步迟迟完成不了,此时会将该follower踢出ISR列表</li> <li>ISR: 与leader同步到了一定程度[当前副本的LEO&gt;=分区HW]的副本集合</li> <li>LEO: 每个副本最后一个offset</li> <li>HW: 所有副本中最小的LEO</li> <li>故障处理机制:
<ul><li>follower故障: follower故障解决之后,应该会清除掉故障之前HW之后的所有数据,重新从leader同步数据</li> <li>leader故障: 首先从ISR列表中选择一个作为新leader,其余follower需要清除掉HW之后的所有数据重新从新leader同步数据</li></ul></li></ul> <p>3、exactly once</p> <p>三种容错语义:</p> <p>at-lest-once: 数据最少一条[数据重复]</p> <p>at-most-once: 数据最多一条[数据丢失]</p> <p>exactly-once: 数据有且仅有一条</p> <p>kafka如何确保exactly once:  通过 ack=-1 + 幂等性 [前提: 生产者不宕机,因为producerid是在生产者启动的时候生成的所以如果producer宕机重启会重新生成一个新的producerid]</p> <p>kafka确保exactly once借鉴的是mysql的主键思想:</p> <p>​	kafka每次发送数据的时候都会带上一个主键[ producerid+parititionid+sequceNumber],broker会缓存该主键,所以后续每个发送数据的时候都会判断当前数据主键在缓存中是否存在,如果存在代表该数据之前已经发送过,当前数据标记无效,如果不存在代表数据没有发送过,正常写入log文件</p> <div class="language-txt line-numbers-mode"><pre class="language-txt"><code>producerid: 生产者的唯一标识,是生产者在启动的时候生成的
partitionid：分区号
sequceNumber: 发到分区的第几条数据			
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><h4 id="_3-消费者"><a href="#_3-消费者" class="header-anchor">#</a> 3. 消费者</h4> <ul><li><p>消费组消费数据的方式: 采用主动拉取数据的方式</p></li> <li><p>分区分配策略[消费者组中的消费者究竟消费哪个分区的数据]</p> <ul><li><p>轮询</p> <ul><li><div class="language-txt line-numbers-mode"><pre class="language-txt"><code>比如: Topic[partition0、partition1、partition2、partition3、partition4]
	  consumer group[consumer1,consumer2]
此时轮询分配:
	consumer1: partition0、partition2、partition4
	consumer2: partition1、partition3、
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div></li></ul></li> <li><p>range</p> <ul><li><p>1、评估每个消费者大概消费几个分区的数据: 分区数/消费者个数</p></li> <li><p>2、确定前几个消费者多消费1一个分区的数据: 分区数%消费者个数</p></li> <li><div class="language-txt line-numbers-mode"><pre class="language-txt"><code>比如: Topic[partition0、partition1、partition2、partition3、partition4]
	 consumer group[consumer1,consumer2]
此时分区分配:
	1、评估每个消费者大概消费几个分区的数据: 5/2 = 2
	2、确定前几个消费者多消费1一个分区的数据: 5%2 = 1
			consumer1: partition0、partition1、partition2
			consumer2: partition3、partition4
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div></li></ul></li> <li><p>粘性分区</p></li></ul></li></ul> <h3 id="_7-3-isr-副本同步队列"><a href="#_7-3-isr-副本同步队列" class="header-anchor">#</a> 7.3 ISR 副本同步队列</h3> <p>​	ISR（In-Sync Replicas），副本同步队列。如果Follower长时间未向Leader发送通信请求或同步数据，则该Follower将被踢出ISR。该时间阈值由replica.lag.time.max.ms参数设定，默认30s。</p> <p>​	任意一个维度超过阈值都会把Follower剔除出ISR，存入OSR（Outof-Sync Replicas）列表，新加入的Follower也会先存放在OSR中。</p> <p>​	Kafka分区中的所有副本统称为AR = ISR + OSR</p> <h3 id="_7-4-数据重复"><a href="#_7-4-数据重复" class="header-anchor">#</a> 7.4 数据重复</h3> <p>去重 = 幂等性 + 事务</p> <p>1.幂等性原理</p> <p>Producer不论向 Broker 发送多少次重复数据，Broker 端都只会持久化一条，保证了不重复</p> <p>重复数据的判断标准，具有<code>&lt;PID, Partition, SeqNumber&gt;</code>相同主键的消息提交时，Broker只会持久化一条，其中 PID 是Kafka每次重启都会分配一个新的，Partition表示分区号，SeqNumber是单调递增的。</p> <p>所以幂等性只能保证的是在单分区单会话内不重复</p> <p>2.幂等性配置参数</p> <table><thead><tr><th><strong>参数名称</strong></th> <th><strong>描述</strong></th></tr></thead> <tbody><tr><td><strong>enable.idempotence</strong></td> <td>是否开启幂等性，默认true，表示开启幂等性。</td></tr> <tr><td><strong>max.in.flight.requests.per.connection</strong></td> <td>1.0.X版本前，需设置为1，1.0.X之后，小于等于5</td></tr> <tr><td><strong>retries</strong></td> <td>失败重试次数，需要大于0</td></tr> <tr><td><strong>acks</strong></td> <td>需要设置为all/-1</td></tr></tbody></table> <p>3.Kafka的事务一共有如下5个API</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token comment">// 1初始化事务</span>
<span class="token keyword">void</span> <span class="token function">initTransactions</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">// 2开启事务</span>
<span class="token keyword">void</span> <span class="token function">beginTransaction</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">ProducerFencedException</span><span class="token punctuation">;</span>

<span class="token comment">// 3在事务内提交已经消费的偏移量（主要用于消费者）</span>
<span class="token keyword">void</span> <span class="token function">sendOffsetsToTransaction</span><span class="token punctuation">(</span><span class="token class-name">Map</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">TopicPartition</span><span class="token punctuation">,</span> <span class="token class-name">OffsetAndMetadata</span><span class="token punctuation">&gt;</span></span> offsets<span class="token punctuation">,</span>
                              <span class="token class-name">String</span> consumerGroupId<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">ProducerFencedException</span><span class="token punctuation">;</span>

<span class="token comment">// 4提交事务</span>
<span class="token keyword">void</span> <span class="token function">commitTransaction</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">ProducerFencedException</span><span class="token punctuation">;</span>

<span class="token comment">// 5放弃事务（类似于回滚事务的操作）</span>
<span class="token keyword">void</span> <span class="token function">abortTransaction</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">ProducerFencedException</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>4.总结</p> <p>（1）生产者角度</p> <ul><li><p>acks设置为-1 （acks=-1）。</p></li> <li><p>幂等性（enable.idempotence = true） + 事务 。</p></li></ul> <p>（2）broker服务端角度</p> <ul><li><p>分区副本大于等于2 （--replication-factor 2）。</p></li> <li><p>ISR里应答的最小副本数量大于等于2 （min.insync.replicas = 2）。</p></li></ul> <p>（3）消费者</p> <ul><li>事务 + 手动提交offset （enable.auto.commit = false）。</li> <li>消费者输出的目的地必须支持事务（MySQL、Kafka）。</li></ul> <h3 id="_7-5-kafka如何保证数据有序-or-怎么解决乱序"><a href="#_7-5-kafka如何保证数据有序-or-怎么解决乱序" class="header-anchor">#</a> 7.5 Kafka如何保证数据有序 or 怎么解决乱序</h3> <p>1）Kafka 最多只保证单分区内的消息是有序的，所以如果要保证业务全局严格有序，就要设置 Topic 为单分区。</p> <p>ps：单分区内，可保证数据有序，但也不是绝对的。例如，某批次的数据发送失败后，进行了重试，就可能出现后边的批次先于它到达的情况。</p> <p>2）如何保证单分区内数据有序？</p> <p>方案一：</p> <p>禁止重试，需设置以下参数：</p> <p>设置 retries 等于 0</p> <p>说明：数据出现乱序的根本原因是：失败重试；那么关闭重试，则可以保证数据有序，可能会导致数据的丢失</p> <p>方案二：</p> <p>启用幂等，需设置以下参数：</p> <p>设置 enable.idempotence = true，启用幂等</p> <p>设置 max.in.flight.requests.per.connection，1.0.X之后，小于等于 5</p> <p>设置 retries，保证其 大于 0</p> <p>设置 acks，保证其为 -1</p> <p>幂等机制保证数据有序的原理如下：</p> <p><img src="https://lskyimage-1306894954.cos.ap-nanjing.myqcloud.com/kafka%2Fidempotence.png" alt=""></p> <h3 id="_7-6-kafka-分区-leader-选举规则"><a href="#_7-6-kafka-分区-leader-选举规则" class="header-anchor">#</a> 7.6 Kafka 分区 Leader 选举规则</h3> <p>​	在ISR中存活为前提，按照AR中排在前面的优先。例如AR[1,0,2]，ISR [1，0，2]，那么Leader就会按照1，0，2的</p> <p>顺序轮询。</p> <h3 id="_7-7-ar-的顺序"><a href="#_7-7-ar-的顺序" class="header-anchor">#</a> 7.7 AR 的顺序</h3> <p>​	如果 Kafka 服务器只有 4 个节点，那么设置 Kafka 的分区数大于服务器台数，在 Kafka 底层如何分配存储副本呢？</p> <p>1）创建16分区，3个副本</p> <p>创建一个新的Topic，名称为second</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --partitions <span class="token number">16</span> --replication-factor <span class="token number">3</span> --topic second
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>查看分区和副本情况。</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic second
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><div class="language-txt line-numbers-mode"><pre class="language-txt"><code>Topic: second4	Partition: 0	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2
Topic: second4	Partition: 1	Leader: 1	Replicas: 1,2,3	Isr: 1,2,3
Topic: second4	Partition: 2	Leader: 2	Replicas: 2,3,0	Isr: 2,3,0
Topic: second4	Partition: 3	Leader: 3	Replicas: 3,0,1	Isr: 3,0,1

Topic: second4	Partition: 4	Leader: 0	Replicas: 0,2,3	Isr: 0,2,3
Topic: second4	Partition: 5	Leader: 1	Replicas: 1,3,0	Isr: 1,3,0
Topic: second4	Partition: 6	Leader: 2	Replicas: 2,0,1	Isr: 2,0,1
Topic: second4	Partition: 7	Leader: 3	Replicas: 3,1,2	Isr: 3,1,2

Topic: second4	Partition: 8	Leader: 0	Replicas: 0,3,1	Isr: 0,3,1
Topic: second4	Partition: 9	Leader: 1	Replicas: 1,0,2	Isr: 1,0,2
Topic: second4	Partition: 10	Leader: 2	Replicas: 2,1,3	Isr: 2,1,3
Topic: second4	Partition: 11	Leader: 3	Replicas: 3,2,0	Isr: 3,2,0

Topic: second4	Partition: 12	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2
Topic: second4	Partition: 13	Leader: 1	Replicas: 1,2,3	Isr: 1,2,3
Topic: second4	Partition: 14	Leader: 2	Replicas: 2,3,0	Isr: 2,3,0
Topic: second4	Partition: 15	Leader: 3	Replicas: 3,0,1	Isr: 3,0,1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><h3 id="_7-8-kafka-日志保存时间"><a href="#_7-8-kafka-日志保存时间" class="header-anchor">#</a> 7.8 Kafka 日志保存时间</h3> <p>默认保存7天；生产环境建议3天。</p> <h3 id="_7-9-kafka-过期数据清理"><a href="#_7-9-kafka-过期数据清理" class="header-anchor">#</a> 7.9 Kafka 过期数据清理</h3> <p>日志清理的策略只有delete和compact两种。</p> <p>1）delete日志删除：将过期数据删除</p> <ul><li>log.cleanup.policy = delete ，所有数据启用删除策略
<ul><li>基于时间：默认打开。以segment中所有记录中的最大时间戳作为该文件时间戳。</li> <li>基于大小：默认关闭。超过设置的所有日志总大小，删除最早的segment。
log.retention.bytes，默认等于-1，表示无穷大。</li></ul></li></ul> <p>如果一个segment中有一部分数据过期，一部分没有过期，怎么处理？ 会以最后的时间为准</p> <p>2）compact日志压缩</p> <p>对于相同 key 的不同 value值，只保留最后一个版本</p> <p>log.cleanup.policy = compact 所有数据启用压缩策略</p> <p><img src="https://lskyimage-1306894954.cos.ap-nanjing.myqcloud.com/kafka%2Fcompact.png" alt=""></p> <p>​	压缩后的 offset 可能是不连续的，比如上图中没有 6，当从这些 offset 消费数据时，将会拿到比这个 offset 大的 offset 对于的消息，实际上会拿到 offset 为 7 的消息，并从这个位置开始消费。</p> <p>​	<strong>这种策略只适合特殊场景，比如消息的 key 是用户 ID，value 是用户的资料，通过这种压缩策略，整个消息集里就保存了所有用户最新的资料</strong></p> <h3 id="_7-10-kafka为什么能高效读写数据"><a href="#_7-10-kafka为什么能高效读写数据" class="header-anchor">#</a> 7.10 Kafka为什么能高效读写数据*</h3> <p>1）Kafka本身是分布式集群，可以采用分区技术，并行度高</p> <p>2）读数据采用稀疏索引，可以快速定位要消费的数据</p> <p>3）顺序写磁盘：顺序写之所以快，是因为其省去了大量磁头寻址的时间。</p> <p>4）页缓存 + 零拷贝技术</p> <p>零拷贝：Kafka的数据加工处理操作交由 Kafka 生产者和 Kafka 消费者处理。<strong>Kafka Broker应用层不关心存储的数据，所以就不用走应用层，传输效率高。</strong></p> <p>PageCache页缓存：Kafka重度依赖底层操作系统提供的PageCache功能。当上层有写操作时，操作系统只是将数据写入PageCache。当读操作发生时，先从PageCache中查找，如果找不到，再去磁盘中读取，实际上PageCache是把尽可能多的空闲内存都当做了磁盘缓存来使用</p> <p><img src="https://lskyimage-1306894954.cos.ap-nanjing.myqcloud.com/kafka%2Fzero_copy.png" alt=""></p> <h3 id="_7-11-自动创建主题"><a href="#_7-11-自动创建主题" class="header-anchor">#</a> 7.11 自动创建主题</h3> <p>​	如果Broker端配置参数auto.create.topics.enable设置为true（默认值是true），那么当生产者向一个未创建的主题发送消息时，会自动创建一个分区数为num.partitions（默认值为1）、副本因子为default.replication.factor（默认值为1）的主题。除此之外，当一个消费者开始从未知主题中读取消息时，或者当任意一个客户端向未知主题发送元数据请求时，都会自动创建一个相应主题。这种创建主题的方式是非预期的，增加了主题管理和维护的难度。生产环境建议将该参数设置为false。</p> <h3 id="_7-12-副本数设定"><a href="#_7-12-副本数设定" class="header-anchor">#</a> 7.12 副本数设定</h3> <p>一般我们设置成2个或3个，很多企业设置为2个。</p> <p>副本的优势：提高可靠性；</p> <p>副本劣势：增加了网络IO传输。</p> <h3 id="_7-13-分区数"><a href="#_7-13-分区数" class="header-anchor">#</a> 7.13 分区数</h3> <p>（1）创建一个只有1个分区的Topic。</p> <p>（2）测试这个Topic的Producer吞吐量和Consumer吞吐量。</p> <p>（3）假设他们的值分别是Tp和Tc，单位可以是MB/s。</p> <p>（4）然后假设总的目标吞吐量是Tt，那么分区数 = Tt / min（Tp，Tc）。</p> <p>例如：Producer吞吐量 = 20m/s；Consumer吞吐量 = 50m/s，期望吞吐量100m/s；
分区数 = 100 / 20 = 5分区
分区数一般设置为：3-10个
分区数不是越多越好，也不是越少越好，需要搭建完集群，进行压测，再灵活调整分区个数。</p> <h3 id="_7-14-增加分区"><a href="#_7-14-增加分区" class="header-anchor">#</a> 7.14 增加分区</h3> <p>1）可以通过命令行的方式增加分区，但是分区数只能增加，不能减少。</p> <p>2）为什么分区数只能增加，不能减少？</p> <p>（1）按照Kafka现有的代码逻辑而言，此功能完全可以实现，不过也会使得代码的复杂度急剧增大。</p> <p>（2）实现此功能需要考虑的因素很多，比如删除掉的分区中的消息该作何处理？</p> <p>（3）反观这个功能的收益点却是很低，如果真的需要实现此类的功能，完全可以重新创建一个分区数较小的主题，然后将现有主题中的消息按照既定的逻辑复制过去即可。</p> <h3 id="_7-15-kafka-中多少个-topic"><a href="#_7-15-kafka-中多少个-topic" class="header-anchor">#</a> 7.15 Kafka 中多少个 Topic</h3> <p>ODS层：2个
DWD层：20个</p> <h3 id="_7-16-kafka-消费者是拉取数据还是推送数据"><a href="#_7-16-kafka-消费者是拉取数据还是推送数据" class="header-anchor">#</a> 7.16 Kafka 消费者是拉取数据还是推送数据</h3> <p>拉取数据。</p> <h3 id="_7-17-kafka-消费端分区分配策略"><a href="#_7-17-kafka-消费端分区分配策略" class="header-anchor">#</a> 7.17 Kafka 消费端分区分配策略</h3> <p>1.Range：对每一个topic而言，同一个topic中先排序，再分配数据，对于所有topic而言容易造成数据倾斜</p> <p>2.RoundRobin：针对集群中所有topic而言，按照 hashcode 进行排序，均匀分配</p> <p>3.粘性分区：</p> <p>​	该分区分配算法是最复杂的一种，可以通过 partition.assignment.strategy 参数去设置，从 0.11 版本开始引入，</p> <p>目的就是在执行新分配时，尽量在上一次分配结果上少做调整，其主要实现了以下2个目标：</p> <p>（1）Topic Partition 的分配要尽量均衡。</p> <p>（2）当 Rebalance 发生时，尽量与上一次分配结果保持一致。</p> <p>注意：当两个目标发生冲突的时候，优先保证第一个目标，这样可以使分配更加均匀，其中第一个目标是3种分配策略都尽量去尝试完成的，而第二个目标才是该算法的精髓所在</p> <h3 id="_7-18-消费者再平衡的条件"><a href="#_7-18-消费者再平衡的条件" class="header-anchor">#</a> 7.18 消费者再平衡的条件</h3> <ol><li>Rebalance 的触发条件有三种</li></ol> <p>（1）当Consumer Group 组成员数量发生变化（主动加入、主动离组或者故障下线等）。</p> <p>（2）当订阅主题的数量或者分区发生变化。</p> <p>2）消费者故障下线的情况</p> <table><thead><tr><th><strong>参数名称</strong></th> <th><strong>描述</strong></th></tr></thead> <tbody><tr><td><strong>session.timeout.ms</strong></td> <td>Kafka消费者和coordinator之间连接超时时间，默认45s。超过该值，该消费者被移除，消费者组执行再平衡。</td></tr> <tr><td><strong>max.poll.interval.ms</strong></td> <td>消费者处理消息的最大时长，默认是5分钟。超过该值，该消费者被移除，消费者组执行再平衡。</td></tr></tbody></table> <p>3）主动加入消费者组</p> <p>在现有集群中增加消费者，也会触发Kafka再平衡。注意，如果下游是Flink，Flink会自己维护offset，不会触发Kafka再平衡。</p> <h3 id="_7-19-指定-offset-消费"><a href="#_7-19-指定-offset-消费" class="header-anchor">#</a> 7.19 指定 Offset 消费</h3> <p>可以在任意offset处消费数据。</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code>kafkaConsumer<span class="token punctuation">.</span><span class="token function">seek</span><span class="token punctuation">(</span>topic<span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h3 id="_7-20-指定时间消费"><a href="#_7-20-指定时间消费" class="header-anchor">#</a> 7.20 指定时间消费</h3> <p>可以通过时间来消费数据。</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token class-name">HashMap</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">TopicPartition</span><span class="token punctuation">,</span> <span class="token class-name">Long</span><span class="token punctuation">&gt;</span></span> timestampToSearch <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">HashMap</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

timestampToSearch<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span>topicPartition<span class="token punctuation">,</span> <span class="token class-name">System</span><span class="token punctuation">.</span><span class="token function">currentTimeMillis</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span> <span class="token operator">*</span> <span class="token number">24</span> <span class="token operator">*</span> <span class="token number">3600</span> <span class="token operator">*</span> <span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

kafkaConsumer<span class="token punctuation">.</span><span class="token function">offsetsForTimes</span><span class="token punctuation">(</span>timestampToSearch<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><h3 id="_7-21-kafka-监控"><a href="#_7-21-kafka-监控" class="header-anchor">#</a> 7.21 Kafka 监控</h3> <p>公司自己开发的监控器。</p> <p>开源的监控器：KafkaManager、KafkaMonitor、KafkaEagle。</p> <h3 id="_7-22-kafka-数据积压"><a href="#_7-22-kafka-数据积压" class="header-anchor">#</a> 7.22 Kafka 数据积压</h3> <p>1）发现数据积压
​	通过Kafka的监控器Eagle，可以看到消费lag，就是积压情况：</p> <p><img src="https://lskyimage-1306894954.cos.ap-nanjing.myqcloud.com/kafka%2Fmsg_press.png" alt=""></p> <p>2）解决</p> <p>（1）消费者消费能力不足</p> <p>①可以考虑增加Topic的分区数，并且同时提升消费组的消费者数量，消费者数 = 分区数。（两者缺一不可）。</p> <p>增加分区数(不可逆)：</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --alter --topic first --partitions <span class="token number">3</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>②提高每批次拉取的数量，提高单个消费者的消费能力。</p> <table><thead><tr><th><strong>参数名称</strong></th> <th><strong>描述</strong></th></tr></thead> <tbody><tr><td><strong>fetch.max.bytes</strong></td> <td>默认Default:	52428800（50 m）。消费者获取服务器端一批消息最大的字节数。如果服务器端一批次的数据大于该值（50m）仍然可以拉取回来这批数据，因此，这不是一个绝对最大值。一批次的大小受message.max.bytes （broker config）or max.message.bytes （topic config）影响。</td></tr> <tr><td><strong>max.poll.records</strong></td> <td>一次poll拉取数据返回消息的最大条数，默认是500条</td></tr></tbody></table> <p>（2）消费者处理能力不行</p> <p>​	①消费者，调整fetch.max.bytes大小，默认是50m。</p> <p>​	②消费者，调整max.poll.records大小，默认是500条。</p> <p>​	如果下游是Spark、Flink等计算引擎，消费到数据之后还要进行计算分析处理，当处理能力跟不上消费能力时，会导致背压的出现，从而使消费的速率下降。</p> <p>​	需要对计算性能进行调优（看Spark、Flink优化）。</p> <p>（3）消息积压后如何处理</p> <p>​	某时刻，突然开始积压消息且持续上涨。这种情况下需要你在短时间内找到消息积压的原因，迅速解决问题。
导致消息积压突然增加，只有两种：发送变快了或者消费变慢了。</p> <p>​	假如赶上大促或者抢购时，短时间内不太可能优化消费端的代码来提升消费性能，此时唯一的办法是通过扩容消费端的实例数来提升总体的消费能力。如果短时间内没有足够的服务器资源进行扩容，只能降级一些不重要的业务，减少发送方发送的数据量，最低限度让系统还能正常运转，保证重要业务服务正常。</p> <p>​	假如通过内部监控到消费变慢了，需要你检查消费实例，分析一下是什么原因导致消费变慢？</p> <p>​	①优先查看日志是否有大量的消费错误。</p> <p>​	②此时如果没有错误的话，可以通过打印堆栈信息，看一下你的消费线程卡在哪里「触发死锁或者卡在某些等待资源」。</p> <h3 id="_7-23-如何提升吞吐量"><a href="#_7-23-如何提升吞吐量" class="header-anchor">#</a> 7.23 如何提升吞吐量</h3> <p>1）提升生产吞吐量</p> <p>（1）buffer.memory：发送消息的缓冲区大小，默认值是32m，可以增加到64m。</p> <p>（2）batch.size：默认是16k。如果batch设置太小，会导致频繁网络请求，吞吐量下降；如果batch太大，会导致一条消息需要等待很久才能被发送出去，增加网络延时。</p> <p>（3）linger.ms，这个值默认是0，意思就是消息必须立即被发送。一般设置一个5-100毫秒。如果linger.ms设置的太小，会导致频繁网络请求，吞吐量下降；如果linger.ms太长，会导致一条消息需要等待很久才能被发送出去，增加网络延时。</p> <p>（4）compression.type：默认是none，不压缩，但是也可以使用lz4压缩，效率还是不错的，压缩之后可以减小数据量，提升吞吐量，但是会加大producer端的CPU开销。</p> <p>none、gzip、snappy和lz4</p> <p>2）增加分区</p> <p>3）消费者提高吞吐量</p> <p>（1）调整fetch.max.bytes大小，默认是50m。</p> <p>（2）调整max.poll.records大小，默认是500条。</p> <h3 id="_7-24-kafka中数据量计算"><a href="#_7-24-kafka中数据量计算" class="header-anchor">#</a> 7.24 Kafka中数据量计算</h3> <p>每天总数据量100g，每天产生1亿条日志，10000万/24/60/60=1150条/每秒钟</p> <p>平均每秒钟：1150条</p> <p>低谷每秒钟：50条</p> <p>高峰每秒钟：1150条 *（2-20倍）= 2300条 - 23000条</p> <p>每条日志大小：0.5k - 2k（取1k）</p> <p>每秒多少数据量：2.0M - 20MB</p> <h3 id="_7-25-kafka压测"><a href="#_7-25-kafka压测" class="header-anchor">#</a> 7.25 Kafka压测</h3> <p>用Kafka官方自带的脚本，对Kafka进行压测。</p> <ul><li>生产者压测：kafka-producer-perf-test.sh</li> <li>消费者压测：kafka-consumer-perf-test.sh</li></ul> <p>1）Kafka Producer压力测试</p> <p>（1）创建一个test Topic，设置为3个分区3个副本</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --create --replication-factor <span class="token number">3</span> --partitions <span class="token number">3</span> --topic <span class="token builtin class-name">test</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>（2）在/opt/module/kafka/bin目录下面有这两个文件。我们来测试一下</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>bin/kafka-producer-perf-test.sh  --topic <span class="token builtin class-name">test</span> --record-size <span class="token number">1024</span> --num-records <span class="token number">1000000</span> --throughput <span class="token number">10000</span> --producer-props bootstrap.servers<span class="token operator">=</span>hadoop102:9092,hadoop103:9092,hadoop104:9092 batch.size<span class="token operator">=</span><span class="token number">16384</span> linger.ms<span class="token operator">=</span><span class="token number">0</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>参数说明：</p> <ul><li><p>record-size是一条信息有多大，单位是字节，本次测试设置为1k。</p></li> <li><p>num-records是总共发送多少条信息，本次测试设置为100万条。</p></li> <li><p>throughput 是每秒多少条信息，设成-1，表示不限流，尽可能快的生产数据，可测出生产者最大吞吐量。本次实验设置为每秒钟1万条。</p></li> <li><p>producer-props 后面可以配置生产者相关参数，batch.size配置为16k。</p></li></ul> <p>输出结果：</p> <div class="language-txt line-numbers-mode"><pre class="language-txt"><code>ap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092 batch.size=16384 linger.ms=0
37021 records sent, 7401.2 records/sec (7.23 MB/sec), 1136.0 ms avg latency, 1453.0 ms max latency.
。。。 。。。
33570 records sent, 6714.0 records/sec (6.56 MB/sec), 4549.0 ms avg latency, 5049.0 ms max latency.
1000000 records sent, 9180.713158 records/sec (8.97 MB/sec), 1894.78 ms avg latency, 5049.00 ms max latency, 1335 ms 50th, 4128 ms 95th, 4719 ms 99th, 5030 ms 99.9th.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>（3）调整batch.size大小</p> <p>（4）调整linger.ms时间</p> <p>（5）调整压缩方式</p> <p>（6）调整缓存大小</p> <p>2）Kafka Consumer压力测试</p> <p>（1）修改/opt/module/kafka/config/consumer.properties文件中的一次拉取条数为500</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>max.poll.records<span class="token operator">=</span><span class="token number">500</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>（2）消费100万条日志进行压测</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>bin/kafka-consumer-perf-test.sh --bootstrap-server hadoop102:9092,hadoop103:9092,hadoop104:9092 --topic <span class="token builtin class-name">test</span>  --messages <span class="token number">1000000</span>  --consumer.config config/consumer.properties
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>参数说明：</p> <ul><li>--bootstrap-server指定Kafka集群地址</li> <li>--topic 指定topic的名称</li> <li>--messages 总共要消费的消息个数。本次实验100万条。</li></ul> <p>输出结果：</p> <div class="language-txt line-numbers-mode"><pre class="language-txt"><code>start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec, rebalance.time.ms, fetch.time.ms, fetch.MB.sec, fetch.nMsg.sec
2022-01-20 09:58:26:171, 2022-01-20 09:58:33:321, 977.0166, 136.6457, 1000465, 139925.1748, 415, 6735, 145.0656, 148547.1418
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>（3）一次拉取条数为2000</p> <p>（4）调整fetch.max.bytes大小为100m</p> <h3 id="_7-26-磁盘选择"><a href="#_7-26-磁盘选择" class="header-anchor">#</a> 7.26 磁盘选择</h3> <p>kafka底层主要是顺序写，固态硬盘和机械硬盘的顺序写速度差不多。</p> <p>建议选择普通的机械硬盘。</p> <p>每天总数据量：1亿条 * 1k ≈ 100g</p> <p>100g * 副本2 * 保存时间3天 / 0.7 ≈ 1T</p> <p>建议三台服务器硬盘总大小，大于等于1T。</p> <h3 id="_7-27-内存选择"><a href="#_7-27-内存选择" class="header-anchor">#</a> 7.27 内存选择</h3> <p>Kafka内存组成：堆内存 + 页缓存</p> <p>1）Kafka堆内存建议每个节点：10g ~ 15g</p> <p>在kafka-server-start.sh中修改</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token string">&quot;x<span class="token variable">$KAFKA_HEAP_OPTS</span>&quot;</span> <span class="token operator">=</span> <span class="token string">&quot;x&quot;</span> <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>
    <span class="token builtin class-name">export</span> <span class="token assign-left variable">KAFKA_HEAP_OPTS</span><span class="token operator">=</span><span class="token string">&quot;-Xmx10G -Xms10G&quot;</span>
<span class="token keyword">fi</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>（1）查看Kafka进程号</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>$ jps
<span class="token number">2321</span> Kafka
<span class="token number">5255</span> Jps
<span class="token number">1931</span> QuorumPeerMain
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>（2）根据Kafka进程号，查看Kafka的GC情况</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>jstat -gc <span class="token number">2321</span> <span class="token function">ls</span> <span class="token number">10</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>参数说明：</p> <p>​	YGC：年轻代垃圾回收次数；</p> <p>（3）根据Kafka进程号，查看Kafka的堆内存</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>jmap -heap <span class="token number">2321</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><div class="language-txt line-numbers-mode"><pre class="language-txt"><code>… …

Heap Usage:
G1 Heap:
   regions  = 2048
   capacity = 2147483648 (2048.0MB)
   used     = 246367744 (234.95458984375MB)
   free     = 1901115904 (1813.04541015625MB)
   11.472392082214355% used
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>2）页缓存：</p> <p>页缓存是Linux系统服务器的内存。我们只需要保证1个segment（1g）中25%的数据在内存中就好。</p> <p>每个节点页缓存大小 =（分区数 * 1g * 25%）/ 节点数。例如10个分区，页缓存大小=（10 * 1g * 25%）/ 3 ≈ 1g</p> <p>建议服务器内存大于等于11G。</p> <h3 id="_7-28-cpu选择"><a href="#_7-28-cpu选择" class="header-anchor">#</a> 7.28 CPU选择</h3> <p>1）默认配置</p> <p>​	num.io.threads = 8  负责写磁盘的线程数。</p> <p>​	num.replica.fetchers = 1 副本拉取线程数。</p> <p>​	num.network.threads = 3  数据传输线程数。</p> <p>2）建议配置
​	此外还有后台的一些其他线程，比如清理数据线程，Controller负责感知和管控整个集群的线程等等，这样算，每个Broker都会有上百个线程存在。根据经验，4核CPU处理几十个线程在高峰期会打满，8核勉强够用，而且再考虑到集群上还要运行其他的服务，所以部署Kafka的服务器一般建议在16核以上可以应对一两百个线程的工作，如果条件允许，给到24核甚至32核就更好。</p> <p>​	num.io.threads = 16  负责写磁盘的线程数。</p> <p>​	num.replica.fetchers = 2 副本拉取线程数。</p> <p>​	num.network.threads = 6  数据传输线程数。</p> <p>服务器建议购买 32核CPU</p> <h3 id="_7-29-网络选择"><a href="#_7-29-网络选择" class="header-anchor">#</a> 7.29 网络选择</h3> <p>网络带宽 = 峰值吞吐量 ≈ 20MB/s ，选择千兆网卡即可。</p> <p>100Mbps单位是bit；10M/s单位是byte ; 1byte = 8bit，100Mbps/8 = 12.5M/s。</p> <p>一般百兆的网卡（100Mbps=12.5m/s）、千兆的网卡（1000Mbps=125m/s）、万兆的网卡（1250m/s）。</p> <p>一般百兆的网卡（100Mbps）、千兆的网卡（1000Mbps）、万兆的网卡（10000Mbps）。100Mbps单位是bit；</p> <p>10M/s单位是byte ; 1byte = 8bit，100Mbps/8 = 12.5M/s。</p> <p>通常选用千兆或者是万兆网卡。</p> <h3 id="_7-30-kafka挂掉"><a href="#_7-30-kafka挂掉" class="header-anchor">#</a> 7.30 Kafka挂掉</h3> <p>在生产环境中，如果某个Kafka节点挂掉。</p> <p>正常处理办法：</p> <p>（1）先看日志，尝试重新启动一下，如果能启动正常，那直接解决。</p> <p>（2）如果重启不行，检查内存、CPU、网络带宽。调优=》调优不行增加资源</p> <p>（3）如果将Kafka整个节点误删除，如果副本数大于等于2，可以按照服役新节点的方式重新服役一个新节点，并执行负载均衡。</p> <h3 id="_7-31-kafka-的机器数量"><a href="#_7-31-kafka-的机器数量" class="header-anchor">#</a> 7.31 Kafka 的机器数量</h3> <p><img src="https://lskyimage-1306894954.cos.ap-nanjing.myqcloud.com/kafka%2Fmachine_num.png" alt=""></p> <h3 id="_7-32-服役新节点退役旧节点"><a href="#_7-32-服役新节点退役旧节点" class="header-anchor">#</a> 7.32 服役新节点退役旧节点</h3> <p>可以通过bin/kafka-reassign-partitions.sh脚本服役和退役节点。</p> <h3 id="_7-31-kafka-单条日志传输大小"><a href="#_7-31-kafka-单条日志传输大小" class="header-anchor">#</a> 7.31 Kafka 单条日志传输大小</h3> <p>​	Kafka对于消息体的大小默认为单条最大值是1M但是在我们应用场景中，常常会出现一条消息大于1M，如果不对Kafka进行配置。则会出现生产者无法将消息推送到Kafka或消费者无法去消费Kafka里面的数据，这时我们就要对Kafka进行以下配置：server.properties。</p> <table><thead><tr><th><strong>参数名称</strong></th> <th><strong>描述</strong></th></tr></thead> <tbody><tr><td><strong>message.max.bytes</strong></td> <td>默认1m，Broker端接收每个批次消息最大值。</td></tr> <tr><td><strong>max.request.size</strong></td> <td>默认1m，生产者发往Broker每个请求消息最大值。针对Topic级别设置消息体的大小。</td></tr> <tr><td><strong>replica.fetch.max.bytes</strong></td> <td>默认1m，副本同步数据，每个批次消息最大值。</td></tr> <tr><td><strong>fetch.max.bytes</strong></td> <td>默认Default:	52428800（50 m）。消费者获取服务器端一批消息最大的字节数。如果服务器端一批次的数据大于该值（50m）仍然可以拉取回来这批数据，因此，这不是一个绝对最大值。一批次的大小受message.max.bytes （broker config）or max.message.bytes （topic config）影响。</td></tr></tbody></table> <h3 id="_7-32-kafka参数优化"><a href="#_7-32-kafka参数优化" class="header-anchor">#</a> 7.32 Kafka参数优化</h3> <p>重点调优参数：</p> <p>（1）buffer.memory 32m</p> <p>（2）batch.size：16k</p> <p>（3）linger.ms默认0  调整 5-100ms</p> <p>（4）compression.type采用压缩 snappy</p> <p>（5）消费者端调整fetch.max.bytes大小，默认是50m。</p> <p>（6）消费者端调整max.poll.records大小，默认是500条。</p> <p>（7）单条日志大小：message.max.bytes、max.request.size、replica.fetch.max.bytes适当调整2-10m</p> <p>（8）Kafka堆内存建议每个节点：10g ~ 15g</p> <p>在kafka-server-start.sh中修改</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token string">&quot;x<span class="token variable">$KAFKA_HEAP_OPTS</span>&quot;</span> <span class="token operator">=</span> <span class="token string">&quot;x&quot;</span> <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>
    <span class="token builtin class-name">export</span> <span class="token assign-left variable">KAFKA_HEAP_OPTS</span><span class="token operator">=</span><span class="token string">&quot;-Xmx10G -Xms10G&quot;</span>
<span class="token keyword">fi</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>（9）增加CPU核数</p> <p>num.io.threads = 8  负责写磁盘的线程数</p> <p>num.replica.fetchers = 1 副本拉取线程数</p> <p>num.network.threads = 3  数据传输线程数</p> <p>（10）日志保存时间log.retention.hours 3天</p> <p>（11）副本数，调整为2</p></div></section> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated: </span> <span class="time">6 months ago</span></div></footer> <!----> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-70334359><li class="level-2" data-v-70334359><a href="/blogs/BigData/Kafka.html#_1-常用命令" class="sidebar-link reco-side-_1-常用命令" data-v-70334359>1. 常用命令</a></li><li class="level-2" data-v-70334359><a href="/blogs/BigData/Kafka.html#_2-kafka压力测试" class="sidebar-link reco-side-_2-kafka压力测试" data-v-70334359>2. Kafka压力测试</a></li><li class="level-2" data-v-70334359><a href="/blogs/BigData/Kafka.html#_3-kafka-架构" class="sidebar-link reco-side-_3-kafka-架构" data-v-70334359>3. Kafka 架构</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_3-1-工作机制" class="sidebar-link reco-side-_3-1-工作机制" data-v-70334359>3.1 工作机制</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_3-2-存储机制" class="sidebar-link reco-side-_3-2-存储机制" data-v-70334359>3.2 存储机制</a></li><li class="level-2" data-v-70334359><a href="/blogs/BigData/Kafka.html#_4-生产者" class="sidebar-link reco-side-_4-生产者" data-v-70334359>4. 生产者</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_4-1-分区策略" class="sidebar-link reco-side-_4-1-分区策略" data-v-70334359>4.1 分区策略</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_4-2-数据可靠性保证" class="sidebar-link reco-side-_4-2-数据可靠性保证" data-v-70334359>4.2 数据可靠性保证</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_4-3-exactly-once-语义" class="sidebar-link reco-side-_4-3-exactly-once-语义" data-v-70334359>4.3 Exactly Once 语义</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_4-4-producer-事务" class="sidebar-link reco-side-_4-4-producer-事务" data-v-70334359>4.4 Producer 事务</a></li><li class="level-2" data-v-70334359><a href="/blogs/BigData/Kafka.html#_5-消费者" class="sidebar-link reco-side-_5-消费者" data-v-70334359>5. 消费者</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_5-1-消费方式" class="sidebar-link reco-side-_5-1-消费方式" data-v-70334359>5.1 消费方式</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_5-2-分区分配策略" class="sidebar-link reco-side-_5-2-分区分配策略" data-v-70334359>5.2 分区分配策略</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_5-3-offset的维护" class="sidebar-link reco-side-_5-3-offset的维护" data-v-70334359>5.3 offset的维护</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_5-4-自动提交offset" class="sidebar-link reco-side-_5-4-自动提交offset" data-v-70334359>5.4 自动提交offset</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_5-5-重置offset" class="sidebar-link reco-side-_5-5-重置offset" data-v-70334359>5.5 重置offset</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_5-6-手动提交offset" class="sidebar-link reco-side-_5-6-手动提交offset" data-v-70334359>5.6 手动提交offset</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_5-7-consumer-事务" class="sidebar-link reco-side-_5-7-consumer-事务" data-v-70334359>5.7 Consumer 事务</a></li><li class="level-2" data-v-70334359><a href="/blogs/BigData/Kafka.html#_6-kafka-高效读写数据" class="sidebar-link reco-side-_6-kafka-高效读写数据" data-v-70334359>6. Kafka 高效读写数据</a></li><li class="level-2" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-总结" class="sidebar-link reco-side-_7-总结" data-v-70334359>7. 总结</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-1-kafka基础架构" class="sidebar-link reco-side-_7-1-kafka基础架构" data-v-70334359>7.1 Kafka基础架构</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-2-kafka原理" class="sidebar-link reco-side-_7-2-kafka原理" data-v-70334359>7.2 Kafka原理</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-3-isr-副本同步队列" class="sidebar-link reco-side-_7-3-isr-副本同步队列" data-v-70334359>7.3 ISR 副本同步队列</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-4-数据重复" class="sidebar-link reco-side-_7-4-数据重复" data-v-70334359>7.4 数据重复</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-5-kafka如何保证数据有序-or-怎么解决乱序" class="sidebar-link reco-side-_7-5-kafka如何保证数据有序-or-怎么解决乱序" data-v-70334359>7.5 Kafka如何保证数据有序 or 怎么解决乱序</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-6-kafka-分区-leader-选举规则" class="sidebar-link reco-side-_7-6-kafka-分区-leader-选举规则" data-v-70334359>7.6 Kafka 分区 Leader 选举规则</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-7-ar-的顺序" class="sidebar-link reco-side-_7-7-ar-的顺序" data-v-70334359>7.7 AR 的顺序</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-8-kafka-日志保存时间" class="sidebar-link reco-side-_7-8-kafka-日志保存时间" data-v-70334359>7.8 Kafka 日志保存时间</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-9-kafka-过期数据清理" class="sidebar-link reco-side-_7-9-kafka-过期数据清理" data-v-70334359>7.9 Kafka 过期数据清理</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-10-kafka为什么能高效读写数据" class="sidebar-link reco-side-_7-10-kafka为什么能高效读写数据" data-v-70334359>7.10 Kafka为什么能高效读写数据*</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-11-自动创建主题" class="sidebar-link reco-side-_7-11-自动创建主题" data-v-70334359>7.11 自动创建主题</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-12-副本数设定" class="sidebar-link reco-side-_7-12-副本数设定" data-v-70334359>7.12 副本数设定</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-13-分区数" class="sidebar-link reco-side-_7-13-分区数" data-v-70334359>7.13 分区数</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-14-增加分区" class="sidebar-link reco-side-_7-14-增加分区" data-v-70334359>7.14 增加分区</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-15-kafka-中多少个-topic" class="sidebar-link reco-side-_7-15-kafka-中多少个-topic" data-v-70334359>7.15 Kafka 中多少个 Topic</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-16-kafka-消费者是拉取数据还是推送数据" class="sidebar-link reco-side-_7-16-kafka-消费者是拉取数据还是推送数据" data-v-70334359>7.16 Kafka 消费者是拉取数据还是推送数据</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-17-kafka-消费端分区分配策略" class="sidebar-link reco-side-_7-17-kafka-消费端分区分配策略" data-v-70334359>7.17 Kafka 消费端分区分配策略</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-18-消费者再平衡的条件" class="sidebar-link reco-side-_7-18-消费者再平衡的条件" data-v-70334359>7.18 消费者再平衡的条件</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-19-指定-offset-消费" class="sidebar-link reco-side-_7-19-指定-offset-消费" data-v-70334359>7.19 指定 Offset 消费</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-20-指定时间消费" class="sidebar-link reco-side-_7-20-指定时间消费" data-v-70334359>7.20 指定时间消费</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-21-kafka-监控" class="sidebar-link reco-side-_7-21-kafka-监控" data-v-70334359>7.21 Kafka 监控</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-22-kafka-数据积压" class="sidebar-link reco-side-_7-22-kafka-数据积压" data-v-70334359>7.22 Kafka 数据积压</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-23-如何提升吞吐量" class="sidebar-link reco-side-_7-23-如何提升吞吐量" data-v-70334359>7.23 如何提升吞吐量</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-24-kafka中数据量计算" class="sidebar-link reco-side-_7-24-kafka中数据量计算" data-v-70334359>7.24 Kafka中数据量计算</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-25-kafka压测" class="sidebar-link reco-side-_7-25-kafka压测" data-v-70334359>7.25 Kafka压测</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-26-磁盘选择" class="sidebar-link reco-side-_7-26-磁盘选择" data-v-70334359>7.26 磁盘选择</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-27-内存选择" class="sidebar-link reco-side-_7-27-内存选择" data-v-70334359>7.27 内存选择</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-28-cpu选择" class="sidebar-link reco-side-_7-28-cpu选择" data-v-70334359>7.28 CPU选择</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-29-网络选择" class="sidebar-link reco-side-_7-29-网络选择" data-v-70334359>7.29 网络选择</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-30-kafka挂掉" class="sidebar-link reco-side-_7-30-kafka挂掉" data-v-70334359>7.30 Kafka挂掉</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-31-kafka-的机器数量" class="sidebar-link reco-side-_7-31-kafka-的机器数量" data-v-70334359>7.31 Kafka 的机器数量</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-32-服役新节点退役旧节点" class="sidebar-link reco-side-_7-32-服役新节点退役旧节点" data-v-70334359>7.32 服役新节点退役旧节点</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-31-kafka-单条日志传输大小" class="sidebar-link reco-side-_7-31-kafka-单条日志传输大小" data-v-70334359>7.31 Kafka 单条日志传输大小</a></li><li class="level-3" data-v-70334359><a href="/blogs/BigData/Kafka.html#_7-32-kafka参数优化" class="sidebar-link reco-side-_7-32-kafka参数优化" data-v-70334359>7.32 Kafka参数优化</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div><div></div><APlayer audio="" fixed="true" mini="true" theme="#f9bcdd" loop="loop" order="random" preload="auto" volume="0.3" mutex="true" lrc-type="0" list-folded="true" list-max-height="250" storage-name="vuepress-plugin-meting" id="aplayer-fixed"></APlayer></div></div>
    <script src="/assets/js/app.03646987.js" defer></script><script src="/assets/js/3.a8134f0a.js" defer></script><script src="/assets/js/1.a394b090.js" defer></script><script src="/assets/js/28.682ad17a.js" defer></script>
  </body>
</html>
