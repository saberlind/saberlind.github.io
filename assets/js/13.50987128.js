(window.webpackJsonp=window.webpackJsonp||[]).push([[13],{536:function(a,t,s){"use strict";s.r(t);var _=s(4),e=Object(_.a)({},(function(){var a=this,t=a.$createElement,s=a._self._c||t;return s("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[s("h2",{attrs:{id:"_1-离线数仓"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-离线数仓"}},[a._v("#")]),a._v(" 1. 离线数仓")]),a._v(" "),s("h3",{attrs:{id:"_1-flume零点漂移"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-flume零点漂移"}},[a._v("#")]),a._v(" 1）Flume零点漂移")]),a._v(" "),s("p",[a._v("​\t对于日志，flume sink写入HDFS时，如果按照时间生成文件，在没有明确指定时间的情况下，会读取服务器时间作为创建文件的依据，这会导致日志的实际生成日期与文件不符。")]),a._v(" "),s("p",[a._v("​\t这种情况下，可以通过拦截器在flume事件头指定timestamp作为文件的创建依据。")]),a._v(" "),s("p",[a._v("​\t所谓零点漂移，就是上述问题的具体表现。即在按天生成日志文件的情况下，一条23:59:59左右生成的日志发送到服务器后可能已经是第二天了，如果没有指定时间，会被写入第二天对应的文件中，这就是所谓的零点漂移。")]),a._v(" "),s("p",[a._v("​\t要解决零点漂移的问题，通常是将日志中记录的日志创建时间提取出来，写入flume事件头的timestamp字段，有了这个字段，flume创建文件时，会依据这个字段创建文件，这种场景很类似spark、flink的事件事件和处理事件。")]),a._v(" "),s("div",{staticClass:"language-java line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[s("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[a._v("@Override")]),a._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("public")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Event")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[a._v("intercept")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Event")]),a._v(" event"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("{")]),a._v("\n\n  "),s("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Map")]),s("span",{pre:!0,attrs:{class:"token generics"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("<")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("String")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("String")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v(" headers "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" event"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[a._v("getHeaders")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(";")]),a._v("\n  "),s("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("String")]),a._v(" log "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("new")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("String")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("event"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[a._v("getBody")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("StandardCharsets")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("UTF_8"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(";")]),a._v("\n\n  "),s("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("JSONObject")]),a._v(" jsonObject "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("JSONObject")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[a._v("parseObject")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("log"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(";")]),a._v("\n\n  "),s("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("String")]),a._v(" ts "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" jsonObject"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[a._v("getString")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"ts"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(";")]),a._v("\n  headers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[a._v("put")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"timestamp"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" ts"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(";")]),a._v("\n\n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("return")]),a._v(" event"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(";")]),a._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("}")]),a._v("\n")])]),a._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[a._v("1")]),s("br"),s("span",{staticClass:"line-number"},[a._v("2")]),s("br"),s("span",{staticClass:"line-number"},[a._v("3")]),s("br"),s("span",{staticClass:"line-number"},[a._v("4")]),s("br"),s("span",{staticClass:"line-number"},[a._v("5")]),s("br"),s("span",{staticClass:"line-number"},[a._v("6")]),s("br"),s("span",{staticClass:"line-number"},[a._v("7")]),s("br"),s("span",{staticClass:"line-number"},[a._v("8")]),s("br"),s("span",{staticClass:"line-number"},[a._v("9")]),s("br"),s("span",{staticClass:"line-number"},[a._v("10")]),s("br"),s("span",{staticClass:"line-number"},[a._v("11")]),s("br"),s("span",{staticClass:"line-number"},[a._v("12")]),s("br"),s("span",{staticClass:"line-number"},[a._v("13")]),s("br")])]),s("h3",{attrs:{id:"_2-flume挂掉及优化"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-flume挂掉及优化"}},[a._v("#")]),a._v(" 2）Flume挂掉及优化")]),a._v(" "),s("ol",[s("li",[s("p",[a._v("增加 restart 属性配置 ，可在一定范围内解决该问题。（稳定运行一个月左右）")])]),a._v(" "),s("li",[s("p",[a._v("如果需要更长时间的稳定运行，需配置第二道保险，使用 crontab -e 增加一个linux 的守护任务 shell 脚本。")])])]),a._v(" "),s("h3",{attrs:{id:"_3-datax空值、调优"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-datax空值、调优"}},[a._v("#")]),a._v(" 3）Datax空值、调优")]),a._v(" "),s("h3",{attrs:{id:"_4-hdfs小文件处理"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-hdfs小文件处理"}},[a._v("#")]),a._v(" 4）HDFS小文件处理")]),a._v(" "),s("p",[a._v("（1）采用har归档(企业中常用)方式，har是类似一种文件压缩方式，它能够将多个小文件打包成一个后缀为.har文件，这样减少namenode内存使用的同时，仍然允许对文件进行透明的访问。Hadoop存档是特殊格式的存档。 Hadoop归档文件映射到文件系统目录。 Hadoop归档文件始终具有* .har扩展名。 Hadoop存档目录包含元数据（以_index和_masterindex的形式）和数据（part- *）文件。 _index文件包含作为归档文件一部分的文件名以及这些文件内的位置。")]),a._v(" "),s("p",[a._v("（2）采用CombineTextInputFormat")]),a._v(" "),s("p",[a._v("（3）有小文件场景开启JVM重用；如果没有小文件，不要开启JVM重用，因为会一直占用使用到的task卡槽，直到任务完成才释放。JVM重用可以使得JVM实例在同一个job中重新使用N次，N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间 （MR引擎可用/Spark引擎不可用）")]),a._v(" "),s("p",[a._v("（4）自己写一个MR程序将产生的小文件合并成一个大文件。如果是Hive或者Spark有merge功能自动帮助我们合并。")]),a._v(" "),s("h3",{attrs:{id:"_5-kafka挂掉"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_5-kafka挂掉"}},[a._v("#")]),a._v(" 5）Kafka挂掉")]),a._v(" "),s("p",[a._v("正常处理办法：")]),a._v(" "),s("p",[a._v("（1）先看日志，尝试重新启动一下，如果能启动正常，那直接解决。")]),a._v(" "),s("p",[a._v("（2）如果重启不行，检查内存、CPU、网络带宽。调优=》调优不行增加资源")]),a._v(" "),s("p",[a._v("（3）如果将Kafka整个节点误删除，如果副本数大于等于2，可以按照服役新节点的方式重新服役一个新节点，并执行负载均衡。")]),a._v(" "),s("h3",{attrs:{id:"_6-kafka丢失"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_6-kafka丢失"}},[a._v("#")]),a._v(" 6）Kafka丢失")]),a._v(" "),s("p",[a._v("如果数据重要不允许丢失，就需要设置手动提交 offset ，并且将 offset 维护到支持事务的关系型数据库中")]),a._v(" "),s("h3",{attrs:{id:"_7-kafka数据重复"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_7-kafka数据重复"}},[a._v("#")]),a._v(" 7）Kafka数据重复")]),a._v(" "),s("p",[a._v("下游幂等")]),a._v(" "),s("h3",{attrs:{id:"_8-kafka消息数据积压"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_8-kafka消息数据积压"}},[a._v("#")]),a._v(" 8）Kafka消息数据积压")]),a._v(" "),s("p",[a._v("消费者消费能力不行")]),a._v(" "),s("ol",[s("li",[a._v("增加分区数(不可逆)")]),a._v(" "),s("li",[a._v("提高每批次拉取的数量")])]),a._v(" "),s("p",[a._v("优化下游消费逻辑")]),a._v(" "),s("h3",{attrs:{id:"_9-kafk乱序"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_9-kafk乱序"}},[a._v("#")]),a._v(" 9）Kafk乱序")]),a._v(" "),s("p",[a._v("导致乱序：")]),a._v(" "),s("ol",[s("li",[a._v("重试")])]),a._v(" "),s("p",[a._v("解决：")]),a._v(" "),s("ol",[s("li",[a._v("禁止重试")]),a._v(" "),s("li",[a._v("启动幂等")])]),a._v(" "),s("h3",{attrs:{id:"_10-kafka顺序"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_10-kafka顺序"}},[a._v("#")]),a._v(" 10）Kafka顺序")]),a._v(" "),s("p",[a._v("Kafka只能保证单分区内的消息的有序的，需要设置 Topic 为单分区")]),a._v(" "),s("p",[a._v("ps：单分区内，如果设置消息失败重试，也可能导致乱序")]),a._v(" "),s("h3",{attrs:{id:"_11-kafka优化-提高吞吐量"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_11-kafka优化-提高吞吐量"}},[a._v("#")]),a._v(" 11）Kafka优化（提高吞吐量）")]),a._v(" "),s("ol",[s("li",[a._v("调整发送消息的缓冲区大小")]),a._v(" "),s("li",[a._v("拉取数据的批次大小")]),a._v(" "),s("li",[a._v("压缩")]),a._v(" "),s("li",[a._v("增加分区")])]),a._v(" "),s("h3",{attrs:{id:"_12-kafka底层怎么保证高效读写"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_12-kafka底层怎么保证高效读写"}},[a._v("#")]),a._v(" 12）Kafka底层怎么保证高效读写")]),a._v(" "),s("ol",[s("li",[a._v("Kafka本身是分布式集群，可以采用分区技术，并行度高")]),a._v(" "),s("li",[a._v("读数据采用稀疏索引，可以快速定位要消费的数据")]),a._v(" "),s("li",[a._v("顺序写磁盘，省去大量寻址时间")]),a._v(" "),s("li",[a._v("页缓存 + 零拷贝技术")])]),a._v(" "),s("p",[a._v("零拷贝：Kafka的数据加工处理操作交由 Kafka 生产者和 Kafka 消费者处理。Kafka Broker应用层不关心存储的数据，所以就不用走应用层，传输效率高。")]),a._v(" "),s("h3",{attrs:{id:"_13-kafka单条日志传输大小"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_13-kafka单条日志传输大小"}},[a._v("#")]),a._v(" 13）Kafka单条日志传输大小")]),a._v(" "),s("h3",{attrs:{id:"_14-hive优化-hive-on-spark"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_14-hive优化-hive-on-spark"}},[a._v("#")]),a._v(" 14）Hive优化（Hive on Spark）")]),a._v(" "),s("h3",{attrs:{id:"_15-hive解决数据倾斜方法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_15-hive解决数据倾斜方法"}},[a._v("#")]),a._v(" 15）Hive解决数据倾斜方法")]),a._v(" "),s("h2",{attrs:{id:"_2-实时数仓"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-实时数仓"}},[a._v("#")]),a._v(" 2. 实时数仓")]),a._v(" "),s("h3",{attrs:{id:"_2-1-状态编程"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-状态编程"}},[a._v("#")]),a._v(" 2.1 状态编程")]),a._v(" "),s("p",[a._v("项目中哪里用到状态编程，状态是如何存储的，怎么解决大状态问题")]),a._v(" "),s("p",[a._v("1）Dim动态分流使用广播状态，新老访客修复使用键控状态")]),a._v(" "),s("p",[a._v("​\t状态中数据少时使用HashMap，状态中数据多时使用RocksDB")]),a._v(" "),s("p",[a._v("2）大状态优化手段")]),a._v(" "),s("p",[a._v("（1）使用rocksdb")]),a._v(" "),s("p",[a._v("（2）开启增量检查点、本地恢复、设置多目录")]),a._v(" "),s("p",[a._v("（3）设置预定义选项为 磁盘+内存 的策略，自动设定 writerbuffer、blockcache等")]),a._v(" "),s("h3",{attrs:{id:"_2-2-反压-重点"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-反压-重点"}},[a._v("#")]),a._v(" 2.2 反压(重点)")]),a._v(" "),s("p",[a._v("项目中哪里遇到了反压，造成的危害，定位解决？")]),a._v(" "),s("p",[a._v("1）项目中反压造成的原因")]),a._v(" "),s("ul",[s("li",[s("p",[a._v("流量洪峰：不需要解决")])]),a._v(" "),s("li",[s("p",[a._v("频繁GC：比如代码中大量创建临时对象")])]),a._v(" "),s("li",[s("p",[a._v("大状态：新老访客修复")])]),a._v(" "),s("li",[s("p",[a._v("关联外部数据库：从Hbase读取维度数据或将数据写入Clickhouse")])]),a._v(" "),s("li",[s("p",[a._v("数据倾斜：keyby之后不同分组数据量不一致")])])]),a._v(" "),s("p",[a._v("2）反压的危害")]),a._v(" "),s("p",[a._v("问题：Checkpoint超时失败导致job挂掉")]),a._v(" "),s("p",[a._v("内存压力变大导致的OOM导致job挂掉")]),a._v(" "),s("p",[a._v("时效性降低")]),a._v(" "),s("p",[a._v("3）定位反压")]),a._v(" "),s("p",[a._v("（1）利用Web UI定位")]),a._v(" "),s("p",[a._v("​\t定位到造成反压的节点，排查的时候，先把operator chain禁用，方便定位到具体算子。\nFlink 现在在UI上通过颜色和数值来展示繁忙和反压的程度。")]),a._v(" "),s("p",[a._v("​\t上游都是high，找到下游第一个为ok的节点就是瓶颈节点。")]),a._v(" "),s("p",[a._v("（2）利用Metrics定位")]),a._v(" "),s("p",[a._v("​\t可以根据指标分析反压： (下游)buffer.inPoolUsage、(上游)buffer.outPoolUsage")]),a._v(" "),s("p",[a._v("​\t可以分析数据传输")]),a._v(" "),s("p",[a._v("4）处理反压")]),a._v(" "),s("p",[a._v("​\t反压可能是暂时的，可能是由于负载高峰、CheckPoint 或作业重启引起的数据积压而导致反压。如果反压是暂时的，应该忽略它。")]),a._v(" "),s("p",[a._v("（1）查看是否数据倾斜")]),a._v(" "),s("p",[a._v('（2）使用火焰图分析看顶层的哪个函数占据的宽度最大。只要有"平顶"（plateaus），就表示该函数可能存在性能问题。')]),a._v(" "),s("p",[a._v("（3）分析GC日志，调整代码")]),a._v(" "),s("p",[a._v("（4）资源不合理造成的：调整资源")]),a._v(" "),s("p",[a._v("（5）与外部系统交互：")]),a._v(" "),s("p",[a._v("​\t写MySQL、Clickhouse：攒批写入")]),a._v(" "),s("p",[a._v("​\t读HBase：异步IO、旁路缓存")]),a._v(" "),s("h3",{attrs:{id:"_2-3-数据倾斜-重点"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-数据倾斜-重点"}},[a._v("#")]),a._v(" 2.3 数据倾斜(重点)")]),a._v(" "),s("p",[a._v("1）数据倾斜现象：")]),a._v(" "),s("p",[a._v("​\t相同Task 的多个 Subtask 中，个别Subtask 接收到的数据量明显大于其他 Subtask 接收到的数据量，通过 Flink Web UI 可以精确地看到每个 Subtask 处理了多少数据，即可判断出 Flink 任务是否存在数据倾斜。通常，数据倾斜也会引起反压。")]),a._v(" "),s("p",[a._v("2）数据倾斜解决")]),a._v(" "),s("p",[a._v("（1）数据源倾斜")]),a._v(" "),s("p",[a._v("​\t比如消费Kafka，但是Kafka的Topic的分区之间数据不均衡")]),a._v(" "),s("p",[a._v("​\t读进来之后调用重分区算子：rescale、rebalance、shuffle等")]),a._v(" "),s("p",[a._v("（2）单表分组聚合（纯流式）倾斜")]),a._v(" "),s("p",[a._v("​\tAPI：利用flatmap攒批、预聚合")]),a._v(" "),s("p",[a._v("​\tSQL：开启MiniBatch+LocalGlobal")]),a._v(" "),s("p",[a._v("（3）单表分组开窗聚合倾斜")]),a._v(" "),s("p",[a._v("​\t第一阶段聚合：key拼接随机数前缀或后缀，进行keyby、开窗、聚合")]),a._v(" "),s("p",[a._v("​\t注意：聚合完不再是WindowedStream，要获取WindowEnd作为窗口标记作为第二阶段分组依据，避免不同窗口的结果聚合到一起）")]),a._v(" "),s("p",[a._v("​\t第二阶段聚合：按照原来的key及windowEnd作keyby、聚合")]),a._v(" "),s("p",[a._v("​\t在我们项目中，用到了Clickhouse，我们可以第一阶段打散聚合后，直接写入Click house，查clickhouse再处理第二阶段")]),a._v(" "),s("h3",{attrs:{id:"_2-4-数据一致性"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-数据一致性"}},[a._v("#")]),a._v(" 2.4 数据一致性")]),a._v(" "),s("p",[a._v("上游：kafka保证offset可重发，kafka默认实现")]),a._v(" "),s("p",[a._v("Flink：Checkpoint设置执行模式为Exactly_once")]),a._v(" "),s("p",[a._v("下游：使用事务写入Kafka，使用幂等写入Clickhouse且查询使用final查询")]),a._v(" "),s("h3",{attrs:{id:"_2-5-flinksql性能比较慢如何优化"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-5-flinksql性能比较慢如何优化"}},[a._v("#")]),a._v(" 2.5 FlinkSQL性能比较慢如何优化")]),a._v(" "),s("p",[a._v("（1）设置空闲状态保留时间")]),a._v(" "),s("p",[a._v("（2）开启MiniBatch")]),a._v(" "),s("div",{staticClass:"language-shell line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-shell"}},[s("code",[a._v("-- sql中开启\n-- 开启\n"),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("set")]),a._v(" table.exec.mini-batch.enabled"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("true"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(";")]),a._v(" \n-- 最大缓存时间\n"),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("set")]),a._v(" table.exec.mini-batch.allow-latency"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[a._v("'5 s'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(";")]),a._v(" \n-- 批次大小\n"),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("set")]),a._v(" table.exec.mini-batch.size"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("1000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(";")]),a._v("\n")])]),a._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[a._v("1")]),s("br"),s("span",{staticClass:"line-number"},[a._v("2")]),s("br"),s("span",{staticClass:"line-number"},[a._v("3")]),s("br"),s("span",{staticClass:"line-number"},[a._v("4")]),s("br"),s("span",{staticClass:"line-number"},[a._v("5")]),s("br"),s("span",{staticClass:"line-number"},[a._v("6")]),s("br"),s("span",{staticClass:"line-number"},[a._v("7")]),s("br")])]),s("p",[a._v("（3）开启LocalGlobal")]),a._v(" "),s("p",[a._v("Tips:开启预聚合需要先开启MiniBatch")]),a._v(" "),s("div",{staticClass:"language-shell line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-shell"}},[s("code",[a._v("-- 开启预聚合\n"),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("set")]),a._v(" table.optimizer.agg-phase-strategy"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("TWO_PHASE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(";")]),a._v("\n")])]),a._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[a._v("1")]),s("br"),s("span",{staticClass:"line-number"},[a._v("2")]),s("br")])]),s("p",[a._v("（4）开启Split Distinct")]),a._v(" "),s("p",[a._v("（5）多维Distinct使用Filter")]),a._v(" "),s("h3",{attrs:{id:"_2-6-kafka分区动态增加-flink监控不到新分区数据导致数据丢失"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-6-kafka分区动态增加-flink监控不到新分区数据导致数据丢失"}},[a._v("#")]),a._v(" 2.6 Kafka分区动态增加，Flink监控不到新分区数据导致数据丢失")]),a._v(" "),s("p",[a._v("设置Flink动态监控kafka分区的参数")]),a._v(" "),s("h3",{attrs:{id:"_2-7-kafka某个分区没有数据-导致下游水位线无法抬升-窗口无法关闭计算"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-7-kafka某个分区没有数据-导致下游水位线无法抬升-窗口无法关闭计算"}},[a._v("#")]),a._v(" 2.7 Kafka某个分区没有数据，导致下游水位线无法抬升，窗口无法关闭计算")]),a._v(" "),s("p",[a._v("注入水位线时，设置最小等待时间")]),a._v(" "),s("h3",{attrs:{id:"_2-8-hbase的rowkey设计不合理导致的数据热点问题"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-8-hbase的rowkey设计不合理导致的数据热点问题"}},[a._v("#")]),a._v(" 2.8 Hbase的rowkey设计不合理导致的数据热点问题")]),a._v(" "),s("p",[a._v("详见Hbase的rowkey设计原则")]),a._v(" "),s("h3",{attrs:{id:"_2-9-redis和hbase的数据不一致问题"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-9-redis和hbase的数据不一致问题"}},[a._v("#")]),a._v(" 2.9 Redis和HBase的数据不一致问题")]),a._v(" "),s("p",[a._v("采用双删：先删除缓存，再更新数据库，当更新数据后休眠一段时间再删除一次缓存。")]),a._v(" "),s("h3",{attrs:{id:"_2-10-双流join关联不上如何解决"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-10-双流join关联不上如何解决"}},[a._v("#")]),a._v(" 2.10 双流join关联不上如何解决")]),a._v(" "),s("p",[a._v("（1）使用interval join调整上下限时间，但是依然会有迟到数据关联不上")]),a._v(" "),s("p",[a._v("（2）使用left join，带回撤关联")]),a._v(" "),s("p",[a._v("（3）可以使用Cogroup+connect关联两条流")]),a._v(" "),s("h2",{attrs:{id:"_3-生产经验"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-生产经验"}},[a._v("#")]),a._v(" 3. 生产经验")]),a._v(" "),s("h3",{attrs:{id:"_3-1-flink提交任务模式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-flink提交任务模式"}},[a._v("#")]),a._v(" 3.1 Flink提交任务模式")]),a._v(" "),s("p",[a._v("项目中提交使用的per-job模式，因为每个job资源隔离、故障隔离、独立调优")]),a._v(" "),s("h3",{attrs:{id:"_3-2-flink任务提交参数"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-flink任务提交参数"}},[a._v("#")]),a._v(" 3.2 Flink任务提交参数")]),a._v(" "),s("p",[a._v("JobManager和TaskManager分别给多少？")]),a._v(" "),s("p",[a._v("JobManager：内存默认1G，cpu默认1核")]),a._v(" "),s("p",[a._v("TaskManager：数据量多的job，例如：Topic_log分流的job可以给8G")]),a._v(" "),s("p",[a._v("​\t\t\t\t数据量少的job，例如：Topic_db分流的job可以给4G")]),a._v(" "),s("p",[a._v("实时:\t并行度与Kafka分区一致，CPU与Slot比 1：3")]),a._v(" "),s("p",[a._v("20M/s -> 3个分区 -> CPU与Slot比 1:3 -> 3个Slot -> Core数1个 -> CPU与内存比 1:4 -> TM 1 slot -> TM 4G资源")]),a._v(" "),s("p",[a._v("JobManager 2G内存 1CPU")]),a._v(" "),s("p",[a._v("平均 一个Flink作业6G内存，2Core")]),a._v(" "),s("h3",{attrs:{id:"_3-3-flink并行度"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-flink并行度"}},[a._v("#")]),a._v(" 3.3 Flink并行度")]),a._v(" "),s("p",[a._v("全局并行度设置和kafka分区数保持一致为5，Keyby后计算偏大的算子，单独指定。")]),a._v(" "),s("h3",{attrs:{id:"_3-4-flink作业checkpoint参数"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-flink作业checkpoint参数"}},[a._v("#")]),a._v(" 3.4 Flink作业Checkpoint参数")]),a._v(" "),s("p",[a._v("Checkpoint间隔：作业多久触发一次Checkpoint，由job状态大小和恢复调整，一般建议3~5分钟，时效性要求高的可以设置秒级别。")]),a._v(" "),s("p",[a._v("Checkpoint超时：限制Checkpoint的执行时间，超过此时间，Checkpoint被丢弃，建议10分钟。")]),a._v(" "),s("p",[a._v("Checkpoint最小间隔：避免Checkpoint过于频繁，可以设置分钟级别。")]),a._v(" "),s("p",[a._v("Checkpoint的执行模式：Exactly_once或At_least_once，选择Exactly_once。")]),a._v(" "),s("p",[a._v("Checkpoint的存储后端：一般存储HDFS。")]),a._v(" "),s("h3",{attrs:{id:"_3-5-迟到数据如何解决"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-迟到数据如何解决"}},[a._v("#")]),a._v(" 3.5 迟到数据如何解决")]),a._v(" "),s("p",[a._v("（1）设置乱序时间")]),a._v(" "),s("p",[a._v("（2）窗口允许迟到时间")]),a._v(" "),s("p",[a._v("（3）侧输出流")]),a._v(" "),s("p",[a._v("生产中侧输出流，需要Flink单独处理，在写入Clickhouse，通过接口再次计算")]),a._v(" "),s("h3",{attrs:{id:"_3-6-实时数仓延迟多少"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-6-实时数仓延迟多少"}},[a._v("#")]),a._v(" 3.6 实时数仓延迟多少")]),a._v(" "),s("p",[a._v("反压，状态大小，资源偏少，机器性能，checkpoint时间都会影响数仓延迟。")]),a._v(" "),s("p",[a._v("一般影响最大就是窗口大小，一般是5s。")]),a._v(" "),s("p",[a._v("如果启用两阶段提交写入Kafka，下游设置读已提交，那么需要加上CheckPoint间隔时间。")]),a._v(" "),s("h3",{attrs:{id:"_3-7-如何处理缓存冷启动问题"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-7-如何处理缓存冷启动问题"}},[a._v("#")]),a._v(" 3.7 如何处理缓存冷启动问题")]),a._v(" "),s("p",[a._v("初次启动，Redis没有缓存数据，大量读请求访问Habse，类似于缓存雪崩")]),a._v(" "),s("p",[a._v("从离线统计热门维度数据，最近三天用户购买，活跃的sku，手动插入Redis。")]),a._v(" "),s("h3",{attrs:{id:"_3-8-如何处理动态分流冷启动问题"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-8-如何处理动态分流冷启动问题"}},[a._v("#")]),a._v(" 3.8 如何处理动态分流冷启动问题")]),a._v(" "),s("p",[a._v("主流数据先到，丢失数据怎么处理？")]),a._v(" "),s("p",[a._v("在Open方法中预加载配置信息到HashMap以防止配置信息后到")]),a._v(" "),s("h3",{attrs:{id:"_3-9-代码升级-修改代码-如何上线"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-9-代码升级-修改代码-如何上线"}},[a._v("#")]),a._v(" 3.9 代码升级，修改代码，如何上线")]),a._v(" "),s("p",[a._v("Savepoint停止程序，通过Savepoint恢复程序。")]),a._v(" "),s("p",[a._v("代码改动较大，savepoint恢复不了怎么办，看历史数据要不要，要从头跑，不要就不使用savepoint恢复直接提交运行。")]),a._v(" "),s("h3",{attrs:{id:"_3-10-checkpoint恢复"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-10-checkpoint恢复"}},[a._v("#")]),a._v(" 3.10 Checkpoint恢复")]),a._v(" "),s("p",[a._v("如果现在做了5个Checkpoint，Flink Job挂掉之后想恢复到第三次Checkpoint保存的状态上，如何操作？")]),a._v(" "),s("p",[a._v("在Flink中，我们可以通过设置externalized-checkpoint来启用外部化检查点，要从特定的检查点（例如第三个检查点）恢复作业，我们需要手动指定要从哪个检查点（需要指定到chk-xx目录）恢复。")]),a._v(" "),s("h3",{attrs:{id:"_3-11-flink内部的数据质量怎么把控的"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-11-flink内部的数据质量怎么把控的"}},[a._v("#")]),a._v(" 3.11 Flink内部的数据质量怎么把控的")]),a._v(" "),s("p",[a._v("内部的数据质量：内部一致性检查点")]),a._v(" "),s("h3",{attrs:{id:"_3-12-实时任务问题-延迟-怎么排查"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-12-实时任务问题-延迟-怎么排查"}},[a._v("#")]),a._v(" 3.12 实时任务问题（延迟）怎么排查")]),a._v(" "),s("p",[a._v("实时任务出现延迟时，可以从以下几个方面进行排查：")]),a._v(" "),s("p",[a._v("（1）监控指标：看是否反压")]),a._v(" "),s("p",[a._v("（2）日志信息：查看任务运行时的日志信息，定位潜在的问题和异常情况。例如，网络波动、硬件故障、不当的配置等等。")]),a._v(" "),s("p",[a._v("（3）外部事件：如果延迟出现在大量的外部事件后，则可能需要考虑其他因素（如外部系统故障、网络波动等）。框架混部，资源争抢！")]),a._v(" "),s("h3",{attrs:{id:"_3-13-维度数据查询并发量"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-13-维度数据查询并发量"}},[a._v("#")]),a._v(" 3.13 维度数据查询并发量")]),a._v(" "),s("p",[a._v("未做优化之前，有几千QPS，做完Redis的缓存优化，下降到几十")]),a._v(" "),s("h3",{attrs:{id:"_3-14-prometheus-grafana监控哪些指标"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-14-prometheus-grafana监控哪些指标"}},[a._v("#")]),a._v(" 3.14 Prometheus+Grafana监控哪些指标")]),a._v(" "),s("p",[a._v("监控Flink任务和集群的相关指标")]),a._v(" "),s("p",[a._v("1）TaskManager Metrics：这些指标提供有关TaskManager的信息，例如CPU使用率、内存使用率、网络IO等。")]),a._v(" "),s("p",[a._v("2）Task Metrics：这些指标提供有关任务的信息，例如任务的延迟时间、记录丢失数、输入输出速率等。")]),a._v(" "),s("p",[a._v("3）Checkpoint Metrics：这些指标提供有关检查点的信息，例如检查点的持续时间、成功/失败的检查点数量、检查点大小等。")]),a._v(" "),s("p",[a._v("4）Operator Metrics：这些指标提供有关Flink操作符的信息，例如操作符的输入/输出记录数、处理时间、缓存大小等。")]),a._v(" "),s("h3",{attrs:{id:"_3-15-不停止任务的情况下改flink参数"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-15-不停止任务的情况下改flink参数"}},[a._v("#")]),a._v(" 3.15 不停止任务的情况下改Flink参数")]),a._v(" "),s("p",[a._v("动态分流")]),a._v(" "),s("h3",{attrs:{id:"_3-16-hbase数据彻底删除"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-16-hbase数据彻底删除"}},[a._v("#")]),a._v(" 3.16 HBase数据彻底删除")]),a._v(" "),s("p",[a._v("hbase中有表，里面的1月份到3月份的数据我不要了，我需要删除它（彻底删除），要怎么做")]),a._v(" "),s("p",[a._v("在HBase中彻底删除表中的数据，需要执行以下步骤：")]),a._v(" "),s("p",[a._v("（1）禁用表")]),a._v(" "),s("p",[a._v("（2）创建一个新表")]),a._v(" "),s("p",[a._v("（3）复制需要保留的数据，将需要保留的数据从旧表复制到新表。")]),a._v(" "),s("p",[a._v("（4）删除旧表")]),a._v(" "),s("p",[a._v("（5）重命名新表")]),a._v(" "),s("p",[a._v("​\t在执行这些步骤之前，建议先进行数据备份以防止意外数据丢失。此外，如果旧表中的数据量非常大，复制数据到新表中的过程可能会需要很长时间。")]),a._v(" "),s("h3",{attrs:{id:"_3-17-flink数据倾斜"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-17-flink数据倾斜"}},[a._v("#")]),a._v(" 3.17 Flink数据倾斜")]),a._v(" "),s("p",[a._v("如果flink程序的数据倾斜是偶然出现的，可能白天可能晚上突然出现，然后几个月都没有出现，没办法复现，怎么解决？")]),a._v(" "),s("p",[a._v("Flink本身存在反压机制，短时间的数据倾斜问题可以自身消化掉，所以针对于这种偶然性数据倾斜，不做处理。")]),a._v(" "),s("h3",{attrs:{id:"_3-18-维度数据改变之后-如何保证新join的维度数据是正确的数据"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-18-维度数据改变之后-如何保证新join的维度数据是正确的数据"}},[a._v("#")]),a._v(" 3.18 维度数据改变之后，如何保证新join的维度数据是正确的数据")]),a._v(" "),s("p",[a._v("（1）采用低延迟增量更新，本身就有延迟，没办法保证完全的正确数据。")]),a._v(" "),s("p",[a._v("（2）如果必须要正确结果，只能直接读取MySQL数据，但是需要考虑并发，MySQL机器性能。")])])}),[],!1,null,null,null);t.default=e.exports}}]);