(window.webpackJsonp=window.webpackJsonp||[]).push([[46],{568:function(e,t,a){"use strict";a.r(t);var p=a(4),s=Object(p.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h2",{attrs:{id:"dag-解决了什么问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dag-解决了什么问题"}},[e._v("#")]),e._v(" DAG 解决了什么问题")]),e._v(" "),a("p",[e._v("DAG 的出现主要是为了解决 Hadoop MapReduce 框架的局限性。那么 MapReduce 有什么局限性呢？")]),e._v(" "),a("p",[e._v("主要有两个：")]),e._v(" "),a("ul",[a("li",[e._v("每个 MapReduce 操作都是相互独立的，HADOOP不知道接下来会有哪些Map Reduce。")]),e._v(" "),a("li",[e._v("每一步的输出结果，都会持久化到硬盘或者 HDFS 上。")])]),e._v(" "),a("p",[e._v("当以上两个特点结合之后，我们就可以想象，如果在某些迭代的场景下，MapReduce 框架会对硬盘和 HDFS 的读写造成大量浪费。")]),e._v(" "),a("p",[e._v("而且每一步都是堵塞在上一步中，所以当我们处理复杂计算时，会需要很长时间，但是数据量却不大。")]),e._v(" "),a("p",[e._v("所以 Spark 中引入了 DAG，它可以优化计算计划，比如减少 shuffle 数据。")])])}),[],!1,null,null,null);t.default=s.exports}}]);